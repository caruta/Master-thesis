\documentclass[a4paper,11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{multirow}
\usepackage{hyphenat}
\usepackage{sectsty}
\usepackage{amsmath}
\usepackage{bm}
%\usepackage[style=alphabetic]{biblatex}
%\usepackage[dvipsnames]{xcolor}
%\sectionfont{\bfseries\Large\raggedright}
\allsectionsfont{\raggedright}
\graphicspath{ {images/} }

%\usepackage[T1]{fontenc}

\def\double{\baselineskip 24pt \lineskip 10pt}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}
\textheight 9.5in \textwidth 6in \oddsidemargin 25pt\topmargin
-40pt

\def\baselinestretch{1.2}
\parskip 0.2cm

%\usepackage[autostyle,italian=giullemets]{csquotes}
%\usepackage[babel]{csquotes}
%\usepackage{biblatex}
%\bibliography{biblio.bib}

\begin{document}

%FRONTESPIZIO

\thispagestyle{empty}
\begin{center}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.3]{logoUniba}
\end{figure}
%\begin{center}
{\normalsize DIPARTIMENTO INTERATENEO DI FISICA \textquotedblleft M. MERLIN"} \\
\vspace{0.5cm}
\hrule \vspace{0.5cm}

%\end{center}
%
%
%% Titolo tesi
%%\vspace{1.0cm}
%\begin{center}
{\bf {\large{Tesi di laurea Magistrale in \\ \textquotedblleft Nuclear, Subnuclear and Astroparticle Physics"}}} \\
\vspace{2cm}
{\bf{\large { \Huge{Search for $\tau \rightarrow 3\mu$ decays\\ using $\tau$ leptons produced \\in D and B mesons decays \\in CMS experiment at LHC\\}}}}
\end{center}

% Relatrici & laureanda
\vspace{3.5cm}
\begin{flushleft}
{ Relatrici:} \\
{\bf Dott.ssa Anna Colaleo} 
\hspace{6.2cm} {Laureanda:} \\
{\bf Dott.ssa Rosamaria Venditti} 
\hspace{5cm} {\bf Caterina Aruta}
\end{flushleft}
%
%% Laureanda
%\begin{flushleft}
%\hspace{11cm} {\bf Laureanda:} \\
%\hspace{11cm} {\bf Caterina Aruta}
%\end{flushleft}

% Anno accademico
\vspace{2cm}
\begin{center}
\hrule \vspace{0.05cm}
\hrule \vspace{0.15cm}
{\bf {\large{Anno Accademico 2018-2019}}} \\
\end{center}

\newpage
%\chapter*{Aknowledgments}
\tableofcontents 

%\chapter*{Introduction}


%%%%%%%%%%%%%%%%%%%%
\chapter{Standard Model and new physics search}

\section{The Standard Model}

\section{Physics Beyond the Standard Model}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{The CMS experiment at LHC}
The Large Hadron Collider (LHC) is currently the world's largest and most powerful particle collider ever built. Its main goal is to explore the physics at the TeV energy scale in order to test the predictions of the Standard Model and eventually reveal some violations than can be a hint of new physics, described by Beyond Standard Model theories.\\
After a description of the accelerator, I will describe in detail of the several experiments located around the LHC ring: the Compact Muon Solenoid (CMS) experiment, which is the one that collected the data used in the analysis described in this thesis.

\section{The Large Hadron Collider}
The LHC is a proton-proton (pp) and heavy ions collider built by the European Organization for Nuclear Research (CERN) between 1998 and 2008 and situated beneath the France-Switzerland border near the city of Geneva \cite{ref30}. The accelerator, along with the detectors, is the product of an impressive effort/work that has required the collaboration of more than 100 countries with over 10 thousand scientists.
LHC is placed in a tunnel of 26.7 km in circumference, previously used for the Large Electron Positron (LEP) collider \cite{ref31}, with an average depth of about 100 metres underground. \\
The whole accelerating system is made up of different stages and a complete scheme is shown in Fig. \ref{fig:LHC_complex}. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{LHC_complex}
	\caption{The different stages of CERN accelerator complex.}
	\label{fig:LHC_complex}
\end{figure}

The LHC tunnel contains two adjacent parallel beam pipes, kept at ultrahigh vacuum, in which the particles travel in opposite directions around the ring and intersect in four points, where the collisions take place.
Around these crossing points the detectors are positioned, in order to record and later analyse all the possible information resulting from the scattering of the beams.\\
The main experiments present at LHC are: \emph{A ToroidaL ApparatuS} (ATLAS) \cite{ref34}, \emph{Compact Muon Solenoid} (CMS)\cite{ref32}, \emph{Large Hadron Collider beauty} experiment (LHCb) \cite{ref36} and \emph{A Large Ion Collider Experiment} (ALICE)\cite{ref35}.\\
The first two are general purpose detectors: after the search and discovery of the Higgs boson by both collaborations in 2012 \cite{ref1} \cite{ref2} \cite{ref3}, their main task is to study the production and the decay of the discovered Higgs boson, to investigate its properties and to check if it is the SM Higgs boson or a Higgs boson of an extension of the SM.
In addition to this, several searches for physics beyond the SM are also part of their physics program, among them searches for supersymmetric particles and further Higgs bosons.\\
The LHCb experiment is specialised on heavy flavour physics: it looks for indirect evidence of new physics in CP violation and rare decays of bottom and charm hadrons, in order to explain the large asymmetry between the amount of matter and anti-matter in the universe.\\
ALICE is a dedicated heavy-ion detector: it searches for evidence of the quark-gluon plasma, a presumed state of matter with asymptotically free quarks and gluons.

The designed LHC centre of mass energy of the collider ($\sqrt{s}$), which is simply the sum of the energies of the 2 interacting beams, for proton-proton collisions is $\sqrt{s}$ = 14 TeV, not yet achieved – now we are at 13 TeV. Such high energy values can be reached, starting from $\sim$450 GeV (the energy that protons have when they are injected in the LHC ring), by accelerating them using radiofrequency cavities, which play also an important role in synchronizing temporally the protons, grouping them into discrete packets called “bunches”. Each of these bunches is made up of about $\mathrm{10^{11}}$ protons and a bunch collision takes place every 25 ns, providing an interaction rate of 40 MHz.
The beams are kept on their circular path with 1232 dipole magnets, while about 392 quadrupole magnets focus spatially the beams. There are also other kind of magnets used to "squeeze" the particles closer together in correspondence of the interactions points to increase the chances of collisions. In total there are about 10000 superconducting magnets, which are constantly kept at a temperature of 1.9 K by a cooling system based on liquid helium.\\
The number of protons contained in each bunch ($N$), together with the number of bunches rotating in the accelerator ($n_{b}\sim$2500), collision frequency ($f$) and the RMS of beam profile in the plane orthogonal to the beam direction ($\sigma_{xy}$), contribute to the \emph{Luminosity} of the machine, which is a parameter used to quantify the performance of a particle accelerator.
The luminosity is defined as the ratio between the event rate $R_{k}$ of a given process k and the cross section characterizing that process $\sigma_{k}$ :
$ \mathrm{L = \frac{R_{k}}{\sigma_{k}}}$

In particular, in the case of a collider with Gaussian-shaped beam bunches crossing with a small angle, like LHC, the luminosity is given by the equation \ref{eq:Lumi}.

\begin{equation}
\mathrm{L = \frac{f\ n_{b}\ N^{2}}{4\ \pi\ \sigma_{xy}^{2}}}
	\label{eq:Lumi}
\end{equation}

For what concerns proton-proton collisions, the LHC was operated at $\sqrt{s}$ = 7 TeV in 2010 and 2011 and at $\sqrt{s}$ = 8 TeV in 2012, during LHC Run I data-taking. In 2013 there was a long shutdown to upgrade the accelerator in order to increase the center of mass energy up to $\sqrt{s}$ = 13 TeV.
In the LHC Run II data-taking, from 2015 to 2018, data were collected at $\sqrt{s}$ = 13 TeV.\\ 

The LHC designed luminosity is $10^{34}\ \mathrm{cm^{-2}\ s^{-1}}$, which was first reached in June 2016 and doubled in 2017.\\
Integrating this parameter with respect to time the \emph{Integrated Luminosity} is obtained, and the goal of a lot of efforts from the physicists working on the accelerator is to maximize this value because the higher the integrated luminosity, the more data is available to analyze and therefore it becomes possible to detect also rare processes (with very low cross section).

Fig. \ref{fig:IntLumi_cumulative_total} show the total integrated luminosity delivered by LHC and recorded by CMS experiment for proton-proton collisions since 2010.

\begin{figure}[h]
 \begin{minipage}[b]{7.5cm}
   \centering
   \includegraphics[width=7.6cm]{IntLumi_cumulative_peryear}
 \end{minipage}
 \ \hspace{1mm} \hspace{1mm} \
 \begin{minipage}[b]{7.5cm}
  \centering
   \includegraphics[width=7.6cm]{IntLumi_cumulative_total}
 \end{minipage}
 \caption{On the right: cumulative luminosity versus day delivered to CMS for the whole 2010-2018 period for pp collisions, shown for different data taking. On the left: cumulative luminosity versus day delivered by LHC (in blue) and recorded by CMS (in orange) for pp collisions from  2010 to 2018. \cite{ref37}
 \label{fig:IntLumi_cumulative_total}}
\end{figure}

In particular, the integrated luminosity for 2017 and 2018 data-taking are displayed in the plots in Fig. \ref{fig:IntLumi1718}.

\begin{figure}[h]
 \begin{minipage}[b]{7.5cm}
   \centering
   \includegraphics[width=7.6cm]{IntLumi2017}
 \end{minipage}
 \ \hspace{1mm} \hspace{1mm} \
 \begin{minipage}[b]{7.5cm}
  \centering
   \includegraphics[width=7.6cm]{IntLumi2018}
 \end{minipage}
\caption{Integrated luminosity delivered by LHC (in blue) and recorded by CMS (in yellow) for pp collisions of 2017 (on the left) and 2018 (on the right) \cite{ref37}.}
\label{fig:IntLumi1718} 
\end{figure}

When two bunches of protons collide in the interaction point, several independent proton-proton interactions can take place, from which particles can originate.
The average number of primary vertices depends on the beam parameters (e.g. number of particles in a bunch, focusing of the bunch, etc...).\\ 
In 2017 this number has been measured by the CMS experiment and corresponds to, on average, 32 interactions per bunch crossing, as shown in Fig. \ref{fig:pileup2017}.\\ 
The presence of many primary vertices per bunch crossing presents a challenge for the event reconstruction, since the particles originating from different primary vertices can be superimposed in the detector. Interactions besides the interaction of interest that one wants to study are referred to as \emph{pileup}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.65]{pileup2017}
	\caption{Pile-up distribution for 2017 pp collisions data.}
	\label{fig:pileup2017}
\end{figure}

\section{The CMS experiment}
The CMS experiment is a multipurpose experiment \cite{ref33}; it is designed to be able to fulfill a large variety of physics goals, ranging from the investigation of the physics underlying the electro-weak symmetry breaking, that has led to the discovery of the Higgs boson in 2012 \cite{ref2} \cite{ref3}, to the exploration of new physics, probing the predictions from BSM theories. \\
The CMS community is very numerous and spread all around the world: it involves more than  4000 scientists in more than 50 countries.

\subsection{The coordinate system}
The CMS coordinate system is right-handed and its origin is at the centre of the detector, which is the nominal interaction point (IP). The x-axis points radially inward to the center of the LHC ring, the y-axis points vertically upward and the z-axis points horizontally along the counter clockwise beam direction. Since the experiment has a cylindrical symmetry, it’s very useful to define cylindrical coordinates to label the position of the particles. In particular are used: a radial coordinate r measured in the x-y plane and 2 angles : the azimuthal angle $\phi$, defined as the angle measured from the x-axis in the x-y plane and the polar angle $\theta$ measured from the z-axis.
However, instead of the polar angle, is used the pseudorapidity $\eta$, defined by the equation \ref{eq:eta}.

\begin{equation}
\mathrm{\eta = - ln\ \bigg( tan\ \frac{\theta}{2} \bigg)}
	\label{eq:eta}
\end{equation}

Therefore the pseudorapidity is null in the x-y plane and infinity for a direction parallel to the beamline. This quantity is preferred over the polar angle because the particle production is constant as a function of $\eta$ and it is Lorentz invariant under boosts along the longitudinal axis.\\
Based on values of pseudorapidity value, the CMS detector can be divided in two main parts: the \emph{barrel} corresponding to the region with $|\eta| <$1.2 and the two \emph{endcaps}, characterized by $|\eta| <$2.4. The CMS geometric acceptance is therefore limited to the region of $|\eta|<$2.4. \\
The energy and momentum measured for $\eta$ = 0 , i.e. transverse to the z-axis, are denoted as $\mathrm{E_{T}}$ and $\mathrm{p_{T}}$ and defined respectively as:
$\mathrm{E_{T} = E\ sin\ \theta}$ and $\mathrm{p_{T} = p\ sin\ \theta}$. \\
Distances in $\phi$ and $\eta$ are denoted $\Delta \phi$ and $\Delta \eta$. These are used to define cones around an axis with a border defined by $\Delta R$, computed as shown in eq. \ref{eq:DeltaR}.

\begin{equation}
\mathrm{\Delta R = \sqrt{(\Delta \phi)^{2} + (\Delta \eta)^{2}}}
	\label{eq:DeltaR}
\end{equation}

In the following, before describing in detail the CMS subdetectors (indicated in Fig. \ref{fig:CMSdet}) going from the inner one, in the barrel, to the most external in the endcaps, I will briefly illustrate the characteristics of the CMS magnet, which has a fundamental role in the measurement of the momentum and charge of particles.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.14]{CMSdet}
	\caption{Overview of whole the CMS detector showing the different subdetectors.}
	\label{fig:CMSdet}
\end{figure}

\subsection{Magnet}
The momenta of charged particles and the sign of their electric charge are determined by the curvature of the particle trajectory in a magnetic field.
To fulfill the required performance of the muon system, in order to be able to determine the sign of muons with very high momentum, up to the order of TeV, CMS chose a very strong magnetic field within a compact volume.
The CMS large superconducting solenoid, made of niobium titanium and cooled down to $\sim$4.5 K with liquid helium, is 12.5 m long and has an inner diameter of 5.9 m \cite{ref44}.
It produces a uniform field in the axial direction and therefore the particles trajectories are bended in the transverse (x-y) plane. In the volume of the inner tracker and calorimeters the field is about $\sim$ 3.8 T, generated by a circulating current of 18 kA.
The return flux is given by an external iron yoke with three layers, and between them the muon system is installed. In this region the magnetic field is about 2T \cite{ref38}.

\subsection{Inner tracker}
The inner tracking system measures the trajectories of the particles in the pseudorapidity region: $\eta < |2.5|$ and therefore is the closest subdetector to the interaction point and operates in the region of highest flux of particles.
For these reasons is necessary to use a technology characterized by very high granularity, good radiation hardness, while keeping to the minimum the amount of material in order to limit multiple coulomb scattering, bremsstrahlung and nuclear interactions.
The silicon technology (Si) has been chosen for the whole tracker \cite{ref39} \cite{ref40}, which is made up of different detectors, as shown in Fig. \ref{fig:InnerTracker}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1.2]{InnerTracker}
	\caption{Schematic view of a half of the inner tracking system, showing the five different kinds of silicon detectors used.}
	\label{fig:InnerTracker}
\end{figure}

In the inner region there is a Pixel detector, with 3 layer in the barrel and 2 in the endcaps, having pixel cells of $\approx$ 100 $\times$ 150 $\mu m^{2}$ size. These detectors are guarantee a spatial resolution of the radius and azimuthal angle r-$\phi$ measurement of about 10 $\mu m$, and 20 $\mu m$ for the z-coordinate measurement that allows very precise measurements, providing a small impact parameter resolution, crucial for good secondary vertex reconstruction \cite{ref41}.
The external part of the tracker is made up of different kind of microstrip detectors. 
In the barrel there are in total 10 layers of detectors, divided into:
\begin{itemize}
	\item \textbf{Tracker Inner Barrel (TIB)} providing a single-point resolution of 23-34 $\mu m$  in the r-$\phi$ direction and 23 $\mu m$  in z, 
	\item \textbf{Tracker Outer Barrel (TOB)} with a resolution of 35-52 $\mu m$ in the r-$\phi$ direction and 52 $\mu m$ in z.
\end{itemize}

In the endcaps there are 9 microstrip layers, divided into:
\begin{itemize}
	\item \textbf{Tracker Inner Disks (TID)}
	\item \textbf{Tracker EndCaps (TEC)}
\end{itemize}

Fig. \ref{fig:Tracker_MaterialBudget} shows the material budget of the CMS tracker in units of radiation lengths as a function of the pseudorapidity $\eta$, as estimated from simulation (with an accuracy better than 10 \%).
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{Tracker_MaterialBudget}
	\caption{Total thickness t of the tracker material traversed by a particle produced at the IP, expressed in units of radiation length $X_{0}$. The contribution to the total material budget of each of the subsystems that comprise the CMS tracker is shown, together with contributions from the beam pipe and from the support tube that surrounds the tracker \cite{ref62}.}
	\label{fig:Tracker_MaterialBudget}
\end{figure}

\subsection{Calorimeters}
\subsubsection*{Electromagnetic Calorimeter}
The CMS electromagnetic calorimeter (ECAL) is a homogeneous calorimeter made of Lead Tungstate ($PbWO_{4}$) scintillating crystals, characterized by a scintillation decay time comparable with the 25 ns time interval between two consecutive bunch crossings.
Moreover, this material is characterized by a small Moliere radius (21.9 mm) and a short radiation length (8.9 mm), that allows good shower containment in a limited space \cite{ref42}.
Crystals have a trapezoidal shape and a length of 230-220 mm, corresponding to 25.8 and 24.7 radiation lengths respectively. The scintillation light is collected by silicon Avalanche Photo-Diodes (APDs) or Vacuum Photo-Triodes (VPTs).\\
The layout of CMS ECAL is shown in Fig. \ref{fig:ECAL}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.27]{ECAL}
	\caption{Layout of the CMS electromagnetic calorimeter, showing the different regions ...}
	\label{fig:ECAL}
\end{figure}

It has a total coverage of $|\eta| <$ 3 and is divided into:
\begin{itemize}
	\item a \textbf{ECAL Barrel (EB)} covering the region 0 $< |\eta| <$ 1.479 and equipped with APDs,
	\item two \textbf{ECAL Endcaps (EE)} in the region 1.479 $< |\eta| <$  3.0, equipped with VPTs.
\end{itemize}
Additional preshower detectors (ES) are installed in front of each endcap in the region with 1.653 $< |\eta| <$ 2.6. They consist of a sampling calorimeter per endcap, made up of two layers of lead radiators to initiate electromagnetic showers from incoming electrons and photons, followed by silicon strip detectors to measure the energy deposit and the transverse shower profile. 
This preshower system is fundamental to identify and reject the $\pi_{0}$ mesons decaying into two photons and to improve the measurement of the position of electrons and photons, because as it has a higher granularity than the EE.

\subsubsection*{Hadronic Calorimeter}
The CMS hadronic calorimeter (HCAL) is a sampling calorimeter, using Brass as absorber material, plastic scintillator tiles as active medium (sandwiched between the absorbers), WaveLength Shifting fibers (WLS) to modify the frequency of the scintillation light and optical fibers to transfer the light to the detectors which are hybrid photodiodes. 
Brass was chosen for its short interaction length and because it is non-magnetic \cite{ref43}.\\
The HCAL is divided in two parts:
\begin{itemize}
	\item a \textbf{HCAL Barrel (HB)} covering the region: $|\eta| <$ 1.4 , 
	\item two \textbf{HCAL Endcaps (HE)} in the region 1.3 $< |\eta| <$ 3.0.
\end{itemize}
Since the absorber depth of the ECAL Barrel and the HCAL Barrel in the solenoid is not enough to contain the whole particle shower, an additional calorimeter, HCAL Outer (HO), is placed as a tail catcher, external with respect to the cryostat and within the return yoke, using the iron as absorber. The shower containment and therefore the energy resolution of the calorimeter are thus improved.\\
The location of HCAL and ECAL detectors with respect to the CMS magnet is shown in Fig. \ref{fig:HCAL}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{HCAL}
	\caption{Location of the different calorimeters with respect to the CMS magnet.}
	\label{fig:HCAL}
\end{figure}

In order to improve the identification of forward jets, which is very important for the rejection of many backgrounds, HB and HE are complemented by a very forward calorimeter (HF), that extends the pseudorapidity coverage from $|\eta| <$ 3.0 up to $|\eta| <$ 5.2. 
It uses a Cherenkov-based, radiation-hard technology (because the particle flux in this very forward region is extremely high) with steel as absorber material and quartz fibres as active medium.
Cherenkov light, emitted by particles in the quartz fibres, is channelled to photomultipliers.
Neutral components of the hadron showers are preferentially sampled in the HF, leading to narrower and shorter hadronic showers.
Moreover, the fibres inside HF are arranged in such a way is possible to distinguish showers generated by electrons and photons, which deposit a large fraction of their energy in the first 22 cm of the calorimeter, from those generated by hadrons, which produce nearly equal signals in both calorimeter segments (respectively long 22 and 143 cm) on average.

\subsection{Muon system}
The main tasks of the CMS muon system are the muon identification and the precise measurement of $p_{T}$ and charge of muons with energies ranging from few GeV up to few TeV. Additionally, it provides a robust trigger for events that involve muons and a precise time measurement of the bunch crossing \cite{ref45} .
The system is placed outside the magnet and the detector stations are integrated into the iron return yokes so that the 3.8 T magnetic field inside the solenoid and the 1.8 T average return field bend the tracks in the transverse plane thus allowing the measurement of their $p_{T}$. 
Furthermore, because of the large amount of material in front of the muon chambers, also due to the presence of the iron return yoke of the magnet, the muon system results to be well shielded from charged particles other than muons, making their identification easier \cite{ref46} \cite{ref47}.  
\\
The muon spectrometer is made up of 3 different kinds of gaseous detectors, which assure the robustness and redundancy of the system. These detectors are: 
\begin{itemize}
	\item \textbf{Drift Tubes (DT)} in the barrel : $|\eta| <$ 1.2
	\item \textbf{Cathode Strip Chambers (CSC)} in the endcaps : 0.9 $< |\eta| <$ 2.4
	\item \textbf{Resistive Plate Chambers (RPC)} in barrel and in the endcaps : 0.9 $< |\eta| <$ 1.6
\end{itemize}
The DTs are used only in the barrel region ($|\eta| <$ 1.2), where the residual magnetic field and the muon and neutron induced background rate are low. 
In the endcaps, on the contrary, there is a higher residual magnetic field and large particle rate, and CSCs are most suitable for these radiation conditions, therefore they are installed up to $|\eta| <$ 2.4. 
Both DTs and CSCs provide a very good spatial resolution for the measurement of the $p_{T}$ of charged particles. 
In addition to them, RPCs are placed in both regions (barrel and endcaps), making the system redundant and therefore more robust. Owing to their very good timing, these detectors are mainly contribute to the trigger.
\\
Moreover, DT, RPC and CSC have different sensitivity to the backgrounds, assuring the robustness of the system. In this region the background is composed mainly by secondary muons produced in $\pi$ and K decays, or coming from punch-through hadrons and from low energy electrons originating after slow neutron capture by nuclei with subsequent photon emission. \\ 
The \emph{R-z} cross section of a quadrant of the CMS muon system is shown in Fig. \ref{fig:MuSystemOld}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{MuSystemOld}
	\caption{A quadrant of CMS muon system with the axis parallel to the beam (z) running horizontally and the radius (R) increasing upward. The three different subdetectors highlighted: Drift Tubes (in yellow) are installed in the Muon Barrel(MB), Cathode Strip Chambers (in green) are placed in the Muon Endcap (ME) and Resistive Plate Chambers (in blue) are present in both, barrel and encaps (and labelled as RB and RE). The dark grey areas are the steel flux-return disks of the magnet.}
	\label{fig:MuSystemOld}
\end{figure}

\subsubsection*{Drift Tubes}
The Muon Barrel system of detectors (MB) is made up of 4 stations arranged in coaxial cylinders around the beamline, interleaved with the iron yoke. It is also divided into five wheels along the beam direction following the five wheels of the return yokes. In total there are 250 drift chambers.\\
The basic element of a DT chamber is the drift cell shown in Fig. \ref{fig:DT}.
It is a tube with a rectangular cross section, filled with an Ar/CO2 mixture (85/15) and operating at a gas gain of $10^5$. 
The cathodes stripes are placed along the shorter sides of the rectangle, while the anode wire is in the middle of the cell. A charged particle passing through the detector, ionizes the gas and the produced electros drift towards the anode wire. Since the drift velocity in the operating conditions in known and constant (because the geometry of the cell guarantees a uniform electric field), from the measurement of the electrons drift time is possible to obtain the position of the ionizing  particle.\\
This kind of drift cell is characterized by a maximum drift time of $\sim$ 400 ns and a single point resolution of 200 $\mu m$ and 150 $\mu m$ in z direction.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{DT}
	\caption{Section of a drift cell of a Drift Tube detector, showing the anode wire and the cathode strips as well as the drift lines and the isochrones \cite{ref69}. }
	\label{fig:DT}
\end{figure}

Each DT is composed of 2 or 3 superlayers (SL), each made of 4 stacked layers of drift cells.  The orientation of the anode wires is different among the SL, in order to provide information regarding different coordinates. In the outer SL the wires are parallel to the beamline while in the inner one they are orthogonal to the beamline: the former allows a track measurement in the plane (r-$\phi$), in which the low residual magnetic field bends the tracks, while the latter measures the z coordinate.

\subsubsection*{Cathode Strip Chambers}

The tracking measurement of muons in the two endcaps is the main task of the Cathode Strip Chambers (CSC) which are arranged in the Muon Endcap(ME) system of detectors  in 4 stations.\\
The CSC is a multi-wire proportional chamber, in which the cathode plane is segmented into strips perpendicular to the wire’s direction. These chambers are operated at a gain of 7 $\times 10^4$, using a gas mixture of $Ar/CO_{2}/CF_{4}$ (40/50/10). Each chamber has a trapezoidal shape and is made of 7 cathode planes stacked together, forming 6 gas gaps $\sim$ 10 mm thick, each containing a plane of anode wires, as displayed in Fig. \ref{fig:CSC1}.\\ 
In Fig \ref{fig:CSC2} it’s possible to see how muons are revealed in this detector: when a muon crosses the chamber, it produces an avalanche in the gas, inducing signals both on the wires and on the cathode strips.
These two contributions are combined in order to obtain the position of the ionizing particle: since the wires give information on the radial coordinate, while the cathode planes are segmented into radial strips orthogonal to the wires.\\
The resulting spatial resolution depends on the CSC station in consideration, but in average is about 80 $\mu m$.

\begin{figure}[h]
 \begin{minipage}[b]{5cm}
   \centering
   \includegraphics[width=5cm]{CSC1}
   \caption{Layout of a CSC showing the 7 trapezoidal layers which form 6 gas gaps with planes of anode wires.}
   \label{fig:CSC1}
 \end{minipage}
\ \hspace{2mm} \hspace{2mm} \
 \begin{minipage}[b]{8.5cm}
  \centering
   \includegraphics[width=8cm]{CSC2}
   \caption{On the top: view of the cross section a gas gap with the anode wires, the cathode plane and of a muon passing through. On the bottom: Scheme of the formation of the signal in the detector due to the avalanche reaching the wire and the induced charge distribution on the cathode strips \cite{ref69}.}
   \label{fig:CSC2}
 \end{minipage}
\end{figure}

\subsubsection*{Resistive Plate Chambers}
The main goal of the 1056 RPC, installed both in the barrel and in the endcaps of CMS, is to provide a fast trigger signal, by adding, at the same time, redundancy to the muon spectrometer.\\
The RPC are gaseous parallel-plate detectors characterized by an excellent time resolution (from 1 ns to 50 ps) and therefore able to provide a precise bunch crossing identification.
A single RPC consist of two parallel planes made of bakelite (a very resistive resin) externally coated with graphite and separated by a 2 mm wide gas gap, filled with a gas mixture of 96.2\% $C_{2}H_{2}F_{4}$ (freon) + 3.5\% $iC{4}H_{10}$ (isobutane) + 0.3\% $SF_{6}$ + water vapour  [ now changed for ecologic reasons! ].

In CMS two RPCs are combined in order to improve the efficiency of detection and the signals produced by the avalanches generated by the ionization of the gas in the passage of a charge particle, are collected on a set of readout aluminum strips, placed between the two chambers, as shown in Fig. \ref{fig:RPC}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{RPC}
	\caption{Schematic view of a dual RPC CMS detector.}
	\label{fig:RPC}
\end{figure}

These kind of detectors can operate in two different modes: a \emph{streamer} mode with a strong electric field that produces localized gas discharges in the region near the passage of the ionizing particle, or an \emph{avalanche} mode, in which the electric field less strong than the previous one. The former mode allows only few counts per unit area while the latter one, because of the reduced charge generated in the ionization, is characterized by an increased counting capacity of the chamber. \\
For this reason in CMS the RPC operate in avalanche mode, allowing the detectors to sustain higher rates. 

\subsection{CMS trigger system}
In CMS about $10^{9}$ interactions take place per second, but data can be written to permanent storage with a maximum rate of 600 Hz. Moreover, the cross section of interesting physics phenomena is very low,  
, making useless to store all the data.
For these reasons a trigger system, able to select only the potential interesting events, has a fundamental importance for the experiment.
The decision to retain or to discard an event has to be taken in less than 25 ns, the time interval between two collisions, which is too small to retrieve data from all the detectors. 
Therefore, CMS uses a multi-level trigger system \cite{ref48} \cite{ref49}, divided into:
\begin{itemize}
	\item \textbf{Level 1 trigger (L1)}: at this level all the data is stored in the pipelined memory buffers for a maximum of 3.2 $\mu s$, which corresponds to the max latency time (i.e. the time needed to transfer the raw data from the detector to the electronics that takes the L1 decision and back + the time needed to take the decision ($\sim$1 $\mu s$)). Only  $10^{5}$ events per second are passed to the next level of trigger. The decision is taken using the raw data coming from the calorimeter and muon detectors, which are the fastest ones.
 	\item \textbf{High Level Trigger (HLT}): a farm with several thousand of processors reconstructs the data and then takes decisions, further reducing the rate to few hundreds of Hz, before they are stored permanently.
\end{itemize}

\subsubsection*{L1 trigger}
The first level trigger is provided by custom programmable electronics (e.g. FPGAs) that combines the information coming from the fastest CMS detectors in order to decide whether to store or to discard an event.\\ 
All the data are temporarily stored in pipeline buffers inside each subdetectors electronics for 3.2 $\mu$s. This is the sum of the time needed to take the decision ($<$ 1 $\mu$s) and the time needed to transfer the data from subdetectors to where the decision is taken and back. If the event is considered interesting, after this time it is moved to a buffer to be stored while waiting to be processed by the HLT. \\
The L1 uses the muon trigger and the calorimeter trigger, which identify “trigger objects”  as e, $\gamma$, jets and muons, and categorize them based on their quality (determined by energy or momentum). The information provided by these two triggers are then combined by the \emph{Global Trigger}, that takes the final decision. The Muon trigger uses the segments coming from DT and CSC and the single hits from the RPC.\\
A complete overview of the CMS L1 trigger is shown in Fig. \ref{fig:L1}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{L1}
	\caption{An overview of the CMS L1 trigger. Data from the calorimeters (HF, HCAL, ECAL) are processed first regionally in a "Regional Calorimeter Trigger" (RCT) and then globally in a "Global Calorimeter Trigger" (GCT). The hits in the muon system (RPC, CSC, DT) are processed either via a pattern comparator or via a system of segment and track-finders and sent to a global muon trigger (GMT). The information from the GCT and GMT is combined in a global trigger (GT), which makes the final trigger decision. This decision is sent to the tracker (TRK), ECAL, HCAL or muon systems (MU) via the trigger, timing and control (TTC) system. The data acquisition system (DAQ) reads data from various subsystems for offline storage.}
	\label{fig:L1}
\end{figure}
\newline Inner tracker data are not used in L1 because of the large number of channels that allows its very high resolution: it would require too much time to read them out.\\
The simplest L1 triggers are in general those based on the presence of one object with a $\mathrm{p_{T}}$ above a predefined threshold (\emph{single-object triggers}) and those based on the presence of two objects of the same type (\emph{di-object triggers}) with either symmetric or asymmetric thresholds. 

\subsubsection*{High Level Trigger}
The entire decision process at this level takes $\sim$100 ms: each processor of the farm works on the reconstruction of one event at a time, using data with full resolution and granularity, eventually from all the subdetectors. There are few hundreds of different HLT paths, that look for the presence of particular objects and signatures in an event. 
In order to minimize the decision time, the selection is made in a sequence of nested logical steps: initially only some parts of the event, the less expensive in computational terms, are reconstructed (e.g. the deposit energy in the ECAL) and a filter is applied in order to decide if the reconstructed objects pass the trigger thresholds. If this is the case, the reconstruction continues with the successive step, otherwise the execution of the path is stopped.

\section{The World LHC Computing Grid}
Events that have fired the HLT are stored and then reprocessed in order to be analysed.
In order to "answer" to the very demanding requirements that storing, processing and analysing the huge amount of data produced at LHC experiments pose, an infrastructures worldwide distributed, called “World LHC Computing Grid” (WLCG) has been created \cite{ref50}.
It is made up of 170 computer centres distributed in 42 countries, with a total of $\sim$1 million of computer cores and 1 exabyte of storage.
The different centres are connected via high-speed networks, as shown in Fig.\ref{fig:WLCG}. These centres are organized in different levels, called \emph{Tiers}, based on the number of computer they own and the performance they "offer", which have therefore different tasks.

The fundamental centre is the \emph{Tier 0} located at CERN. 
Its task is to collect all the (raw) data from LHC experiments and to organise it in different groups (Primary Datasets) according to the trigger path with which they were acquired.  Moreover, it convert the raw data into a data format useful for analysis: RECO and AOD (Analysis Object Data). \\
The converted data are transferred and shared among \emph{Tier 1} centres (13 around the world), where they are reconstructed. \\
\emph{Tier 2} centres are instead used for single users analysis. \\
In particular, the Tier 2 present in Bari (\emph{ReCaS}) has been extensively used for the analysis described in this thesis.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{WLCG}
	\caption{Schematic representation of the WLCG distributed infrastructure. It is organized in different levels, \emph{Tiers}, connected via high-speed networks.}
	\label{fig:WLCG}
\end{figure}

%\section{LHC and CMS upgrades}
%
%The schedule of the upgrades of LHC is shown in Fig. \ref{fig:LHC_upgrade}.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[scale=0.28]{LHC_upgrade}
%	\caption{LHC upgrade schedule bla bla.}
%	\label{fig:LHC_upgrade}
%\end{figure}


\chapter{Event reconstruction in CMS}
The reconstruction of an event produced in the collision of particles consists in putting together all the information coming from the different detectors, in order to obtain the trajectory of the particles produced in the collision, to identify them (photon, electron, muon, charged hadron, neutral hadron) and to measure their characteristic quantities (momentum, energy, etc...).\\
Each kind of particle leaves in the different subdetectors a particular signature, as shown in Fig. \ref{fig:CMS_slice}, which makes the particle identification possible. 
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{CMS_slice}
	\caption{Different signatures of particle in the CMS detector. The trajectory of the particles that interact with the tracker are drawn in full line: electrons in red, muons in light blue and charged hadrons in green. The traectories of neutral particles, that interact only with the calorimeters, are indicated with dashed lines: neutral hadrons in green and photons in blue\cite{ref60}.}
	\label{fig:CMS_slice}
\end{figure}
\newline For example, photons are detected from ECAL energy clusters not corresponding to any signal in the tracker, while electrons leave a clear signal in the tracker with a linked ECAL energy clusters, or to possible bremsstrahlung photons emitted along the way through the tracker material.
Muons, on the other hand, are clearly identified as a signal in the tracker consistent with a track or several hits in the muon system. 
Neutral hadrons are identified as HCAL energy clusters not linked to any charged hadron trajectory, or as ECAL and HCAL energy excesses with respect to the expected charged hadron energy deposit [cambia questa frase]
If in the passage through the active material of the detectors the particle interacts, a signal is produced and is recorded as a point in space, called \emph{recHit}. All the recHits are then connected together to reconstruct the particle trajectory.

\section{Particle Flow algorithm}
The Particle Flow algorithm \cite{ref60} puts together the information of all subdetectors in order to identify stable particles as muons, electrons, photons, charged and neutral hadrons. Additionally, from all these info the \emph{Missing Transverse Energy} (MET) can be determined [cos'è? - determined by the absolute value of the vector sum of transverse momenta of all identified particles and reconstructed jets... ] [defined as the negative vectorial sum of the transverse momenta of all the visible particles in the event and it represents the transverse momentum which escapes detection, leaving an imbalance in the transverse plane:
$E_{\ T}^{\ miss} = - \sum_{i} p_{T}^{i}$,  \cite{ref61} , which is important to establish the presence (and also the energy and direction) of particles escaped w/o leaving any signal, as neutrinos or neutral hadrons, and the jets (coming from the fragmentation of quarks) can be reconstructed.
The first step of this algorithm is to connect all the elements between the subdetectors, trying to combine them in closest pairs in the transverse plane.
After that, (i.e. after having produced this kind of “blocks” of elements that will be used to “construct” the event. As the different particles are identified by connecting all their “footprints”, the elements present among the blocks, associated to their passage, are removed from the collection.), 
[First, this algorithm reconstructs the fundamental elements which are charged-particle tracks and clusters in the calorimeters. Then, these elements are linked to blocks. These those blocks are interpreted as particles. The reconstruction and identification performance of jet, tau and missing transverse energy is improved by using the particle-flow algorithm.]
Order followed in the reconstruction:
\begin{itemize}
	\item Muons
	\item Electrons and bremsstrahlung photons 
	\item charged hadrons, neutral hadrons or photons
\end{itemize}

The muons candidates are the first to be reconstructed and then all the elements (tracks and clusters) associated to them are removed from the “available blocks”.
The second kind of particles to be identified and reconstructed are the electrons and bremsstrahlung photons ( = Energetic and isolated photons, converted or unconverted ).
! At this level tracks with high pT uncertainties are masked !
Then, the last step consists of a cross identification of the remaining blocks, which can be charged hadrons, neutral hadrons or photons. Usually hadrons produce secondary particles by interacting in the tracker material via nuclear reactions. When all the blocks have been identified and processed, global event description is created and all the parts of the events are reprocessed.

\section{Tracks and Primary Vertex reconstruction}

\subsection{Tracking of charged particles }
\label{Track charged particles}
The main goal of the reconstruction of the tracks of charged particles is the evaluation of their momentum. It is possible by considering the bending of their trajectories in the magnetic field, due to the Lorentz force: 
F = -….
Knowing the value of the magnetic field B in each point of the space, the particle momentum in a point is estimated by evaluating the tangent to the trajectory in that point (which is a velocity) and then multiplying it by the mass if the particle and its Lorentz factor $\gamma$ :
$\mathrm{p = m\ \gamma\ v}$

However, life is not so simple! there are some factors that makes the estimate more complicate and that has to be taken into account because they modify the momentum and direction of the particles:
-	The inhomogeneity of the magnetic field
-	Energy loss in the detectors
-	Multiple Coulomb scattering that modifies the trajectory (check!)

Regarding the last point, if the particle crosses a sufficient thickness of material, the distribution of the values of deflection angle is a Gaussian centered at zero.
The second point, instead, can be evaluated through the Bethe-Bloch formula, that describes the mean rate of energy loss because of the ionization of the atoms of the material.

[ eq. Bethe Bloch and plot ? / vedi pagg. 51 – 52 Radogna]

\subsubsection*{From RecHits to tracks: how to connect the dots}
After the local reconstruction of the RecHits collected by the silicon detectors of the tracker (with the estimation, for each detector layer, of the particle positions and uncertainties) is carried out, 
the procedure to obtain tracks from the RecHits is independent of the type of silicon subdetector and is characterized by some precise logical steps:
\begin{enumerate}
	\item Seed generation
	\item Track finding 
	\item Track fitting
	\item Track selection
\end{enumerate}
The passage from the RecHits to tracks is performed by using the CMS tracking software referred to as Combinatorial Track Finder (CTF) \cite{ref62}.
It allows pattern recognition and track fitting to occur in the same framework \cite{ref64}.  
The procedure goes on in an iterative way, developed to reduce and simplify the combinatorial complexity that naturally arises.
In fact the initial iterations of the CTF search for tracks that are easiest to recognize (e.g. the ones which large pT, which are quite isolated) and after each iteration, hits associated with tracks are removed, simplifying the search for more difficult classes of tracks (e.g. low pT tracks).
The four steps through which each iteration proceeds are explained in more detail in the following…

\subsubsection{Seed generation}
Seed generation provides initial track candidates for an preliminary estimation of the trajectory, its parameters and its uncertainties. 
Inside the tracker the magnetic field is almost uniform therefore the trajectories of charged particles are helicoidal and need five parameters ( = 3 position coordinates [x,y,z] + the angle the tangent to the trajectories makes with respect to the detector [$\lambda$] + the ratio between electric charge and the momentum of the particle [q/p]) to be uniquely defined. 

 Immagine esemplificativa (come quella di Raffaella ?)
 
In order to extract these parameters three 3D hits or two 3D hits + 1 constraint (e.g. on the trajectory origin: hp that the particle originated close to the beam spot) are needed.
Seeds are constructed in the inner part of the tracker and the track candidate is built outwards. 

\subsubsection*{Track finding}
The estimation of the values of the five parameters needed to define the trajectory is performed using linear fitting algorithms, like the “Kalman filter” (KF) \cite{ref66} \cite{ref63} 

This filter acts iteratively, taking as starting parameters the coarse ones provided by the trajectory seed and updating them by adding hits from successive detector layers, to build track candidates \cite{ref65}. 
In particular, firstly are determined the layers which are compatible with the initial seed trajectory and then the trajectory is extrapolated to these layers, according to the equation of motion of a charged particle in a constant magnetic field, taking into account also the multiple coulomb scattering and energy loss in the material.
The five track candidates with the best normalized $\chi^2$ found at each layer are then propagated to the next compatible layers, until the outermost layer is reached, or a terminating condition is satisfied.
At the end of this stage, to each trajectory is associated a collection of hits and an estimate of the track parameters.

\subsubsection*{Track fitting}
The full information about the trajectory is only available when all hits are known, and the estimate can be biased by constraints applied during the seeding stage. 
For this reason, the trajectory is refitted using the KF, which is initialized with the parameters coming from a preliminary fit of the innermost hits of the track.
Then the fit goes on iteratively, from the inside outwards, through all of the hits, and the track trajectory is updated after the progressive addition of  new hits (along with the estimated hit position uncertainty).

\subsubsection*{Track selection}
The previous reconstruction step (the track fitting) produces several “fake tracks” = tracks not associated with a simulated particle.
[ A reconstructed particle is associated with a simulated track if at least 75\% of the hits assigned to the reconstructed track originate from the simulated particle. (?? Riformula)] 
To avoid fake tracks, the tracks selection is based on (if they fit has a good $\chi^2$/ndf): 
-	number of layers that have hits [ fraction of fake tracks decreases exponentially with the increasing if this quantity) 
-	compatibility with a primary interaction vertex 
According to the “number of criteria” that a track fulfills, the tracks can be “high-purity tracks” if they satisfy the more stringent criteria or “loose tracks”, which fulfill only minimum requirements.
At the end of the selection, the remaining tracks are merged into a single collection.

\subsection{Tracking efficiency}
The tracking efficiency is defined as the number of matched reconstructed tracks divided by number of simulated tracks, and it is a measure of the performance of the detector. For the inner tracker, the muon-tracking efficiency is shown as function of $\mathrm{p_{T}}$ and $\eta$ of the muon in Fig. \ref{fig:TrackerEff} and as a function of the number of primary vertices in Fig. \ref{fig:TrackerEff_Nvertices}, using 2017 data collected at 13 TeV. \\

\begin{figure}[h!]
 \begin{minipage}[b]{7.5cm}
   \centering
   \includegraphics[width=7.6cm]{TrackerEff_pT}
 \end{minipage}
 \ \hspace{1mm} \hspace{1mm} \
 \begin{minipage}[b]{7.5cm}
  \centering
   \includegraphics[width=7.6cm]{TrackerEff_eta}
 \end{minipage}
\caption{Muon tracking efficiency as a function of the transverse momentum (on the left) and of $\eta$ of the muon (on the right). Data (collected in 2017 at a center of energy of 13 TeV) are shown with black dots while the simulation with light blue rectangles. The uncertainties shown are statistical "-/+ 1" $\sigma$" \cite{ref62a}. These efficiencies were measured with a tag-and-probe technique on $\mathrm{Z\rightarrow\mu^{+}\mu^{-}}$ \cite{ref116} \cite{ref69}.}
\label{fig:TrackerEff} 
\end{figure}

\begin{figure}[h!]
   \centering
   \includegraphics[width=7.6cm]{TrackerEff_Nvertices}
	\caption{Muon tracking efficiency as a function of the number of primary vertices. Data (collected in 2017 at a center of energy of 13 TeV) are shown with black dots while the simulation with light blue rectangles. The uncertainties shown are statistical "-/+ 1" $\sigma$" \cite{ref62a}. This efficiency was measured with a tag-and-probe technique on $\mathrm{Z\rightarrow\mu^{+}\mu^{-}}$ \cite{ref116} \cite{ref69}.}
	\label{fig:TrackerEff_Nvertices} 
\end{figure}

\subsection{Primary Vertex reconstruction}
Using the reconstructed tracks, it is possible to reconstruct the Primary Vertices (PV), i. e. the vertices of the proton-proton interactions in each event, including the ones originating from pileup collisions.
This reconstruction process consists of 3 steps:
\begin{enumerate}
	\item Selection of the tracks that result to be consistent with being produced promptly in a primary interaction.
	\item These tracks are clustered according to their z-coordinate at the point of closest approach to the centre of the beam spot
	\item All the vertices containing at least 2 tracks are fitted with an “Adaptive Vertex Fitter” \cite{ref68}/ This provides an estimate of the vertex parameters (position coordinates, number of degrees of freedom, indicators that estimate the efficiency of the fit). The primary-vertex resolution depends strongly on the number and on the momenta of tracks used in the fit. 
\end{enumerate}

- - Secondary vertices reco ?? (vedi Vanff pag. 124 etc)
\newpage
\section{Muon reconstruction}
A good muon reconstruction and identification is fundamental for many physics searches carried out at CMS and, in particular, it has also a very important role for the analysis described in this thesis. Because of the central importance that muons have in CMS, several algorithms have been developed for their reconstruction, in order to best fulfill the specific needs of different analysis \cite{ref46}. \\
In general, muons are reconstructed using data coming from the muon system and the inner tracker.

The entire muon reconstruction consists of 3 stages (the two possibilities at stage 3 are alternative):
\begin{enumerate}
	\item \textbf{Local muon reconstruction}: data are reconstructed as RecHits in each muon chamber. Then, in DT and in CSC the RecHits are fitted to segments.
	\item \textbf{Stand-alone muon reconstruction}: the RecHits in RPC and the segments in DT and CSC are grouped in seed and fitted to “stand-alone muon” tracks, using a Kalman-filter technique.
	\item	 \begin{enumerate}
				\item \textbf{Global muon reconstruction [outside-in]}: for each stand-alone muon track a matching in the inner tracker is searched for. If this track is found, the hits of the two tracks are combined and a global fit is performed with the Kalman filter, resulting in a “global muon” track. %[ using the KF technique]
				\item \textbf{Tracker muon reconstruction [inside-out]}: tracks in the inner tracker with $\mathrm{p_{T}}>$0.5 GeV/c and a total momentum p$>$2.5 GeV/c are extrapolated to the muon system. If at least one matching muon segment (from DT or CSC) is found, the extrapolated track is referred to as a “tracker muon” track. The matching between the track and the segment is done using as reference coordinate system the muon chamber one. In particular, it is required that the extrapolated track is less than 3 cm, in x coordinate, far from the corresponding muon segment.
			\end{enumerate}
\end{enumerate}
The global reconstruction, which uses stand-alone muon tracks, gives a very good momentum resolution for muons with high $\mathrm{p_{T}}$ values: $\mathrm{p_{T} \geq}$ 200 GeV.\\ 
On the other hand, the tracker reconstruction is more efficient for muons with low transverse momentum, $\mathrm{p_{T} \leq}$ 5 GeV, because it requires only 1 segment in the muon chambers, which is usually, for tracker muons that are not global muons, in the innermost muon station.\\
In any case, owing to the high efficiency of the tracker track and muon segment algorithms, the 99\% of the muons produced within the CMS geometrical acceptance and having sufficiently high momentum are reconstructed either as a global muon track or as a tracker muon track, and very often as both \cite{ref46}.\\
When the reconstruction process is completed, global muons and tracker muons that share the same tracker track are merged into a single candidate.

An example of a reconstructed event involving 4 muons is shown in Fig.\ref{fig:4muRECO}.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.22]{4muRECO}
	\caption{Longitudinal (on the left) and transverse (on the right) views of an event in which four muons were reconstructed. Three of them were identified by the DTs and RPCs, the fourth one by the CSCs. The short black segments in the muon system show fitted muon-track segments, while the short red horizontal lines indicate the positions of RPC hits. The energy deposited in the calorimeters is indicated with red and blue bars, respectively for ECAL and HCAL \cite{ref69}.}
	\label{fig:4muRECO}
\end{figure}

\subsection{Local muon reconstruction}
The trajectory of the muon is built starting from the recHits on the sub-detectors layers ( ideally one recHit per layer should be produced). At this level the reconstruction depends on the specific type of muon chamber considered.\\ In the following, the local reconstruction procedure for kind of sub-detectors of the muon system will be treated in more detail.

\subsubsection*{Drift Tubes}
Inside the Drift Tubes, the footprints, left by the muons passing through them, are 1D hits in the drift cells. From the measurement of the drift time, using time-to-digital converter (TDC) registers, it is possible to obtain only information on the distance of the particle from the anode wire, but with a left/right ambiguity. Therefore, a single hit doesn't provide any hint regarding the position of the particle along the wire.\\ 
However, since a DT chamber consists of three \emph{superlayers}, each comprising four staggered layers of parallel drift cells, the wires in each layer are oriented so that two of the superlayers measure the muon position in the bending plane (r -$\phi$) and one superlayer measures the position in the longitudinal plane (r-$\theta$)\cite{ref46}. 
This is the reason why hits segments are reconstructed separately in the r -$\phi$ and r-$\theta$ planes and then the two projections are combined to obtain information about the third coordinate (z).\\
The final 3D segment has a angular resolution of $\sim$ 0.7 mrad in $\phi$ and $\sim$ 6 mrad in $\theta$.

\subsubsection*{Cathode Strip Chambers}
In Cathode Strip Chambers it is possible to have information on the position of the muon by combining signals from the cathode strips (which are radial, so they measure the angle $\phi$) and from the anode wires (that are orthogonal to the strips, providing a measurement of the radial coordinate $r$). \\
Therefore, a 2D information is available for each layer: the info from each layer are then combined to reconstruct a 3D line segment. \\
The position resolution of segments varies between 50-250 $\mu$m, depending on the CSC station considered.

\subsubsection*{Resistive Plate Chambers}
In Resistive Plate Chambers the information on the position of the muon is provided by the signal induced on the readout strips, which are aligned with $\eta$. The fired adjacent strips are clustered and the exact position of the muon is estimated computing the center of gravity of the charge shared among them.\\ This provides a resolution of few cm in the $\phi$ coordinate.
%[Errors are computed under the same assumption of flat probability: length of the cluster divided by $\sqrt{12}$ ]

\subsection{Stand-alone muon reconstruction}
In order to build the seeds, a pattern of segments is searched for, inside the muon stations DT (for barrel) and the CSC (for endcaps). If it is found, the $\mathrm{p_{T}}$ of the seed candidate is estimated.\\
Then, the stand-alone track reconstruction uses the KF method iteratively to extend the track, starting from the candidate seeds. 
At each iteration of the algorithm, the trajectory parameters are updated and the reconstruction is carryed out in the same way it is done for tracker tracks.  After that all the hits are fitted and the fake trajectories are removed, the remaining tracks are extrapolated to the point of closest approach to the beam line. 

\subsection{Global muon reconstruction}
The global muon reconstruction starts after the completion of indepedent reconstruction of the stand-alone tracks and the inner tracker tracks. \\
The track matching of a stand-alone track to a tracker track consists of 2 steps:
\begin{enumerate}
	\item the definition of a region of interest(ROI) in the parameter space, that roughly corresponds to the stand-alone muon track, and the selection of the subset of the tracker tracks inside this region. The determination of the ROI is based on the stand-alone muon parameters, with the assumption that the muon originates from the interaction point. 
	\item the iteration of the previous procedure applying more stringent criteria to choose the best tracker track to be combined with the stand-alone muon. In order to do this, the stand-alone muon and the tracker tracks are propagated onto the same plane and the global track with the best $\chi^2$ is searched. If the matching fails, the reconstruction is stopped, and no global track is produced.
\end{enumerate}
The final step in the reconstruction of a global muon track is the matching of it with the energy deposits in the calorimeters.

\subsection{Tracker muon reconstruction}
The tracker reconstruction consists in extrapolating the tracker tracks to the muon system, taking into account all the possible contributions from the dishomogeneity of the magnetic field, the average expected energy losses of the particle and multiple Coulomb scattering in the detector material.
If there is a suitable match between a tracker track and a stand-alone muon track, then the default global fit algorithm combines hits from the tracker and the stand-alone muon track and performs a final fit over all hits.

\subsection{Muon Identification}
Particles which are detected as muons can be produced in CMS from different sources and therefore can be characterized by various features \cite{ref69}.
In general, they can be distinguished in: 
\begin{itemize}
	\item \textbf{Prompt muons}: The great majority of muons // They are the muons arising either from decays of W, Z, and promptly produced quarkonia states, or other sources such as Drell-Yan processes or top quark production. 
	\item \textbf{Muons from heavy flavor}: Muons coming from the decay of a tau lepton or from beauty or charmed mesons. 
	\item \textbf{Muons from light flavor}: Muons coming from the decay of light hadron decays (e.g. $\pi$ and K) or, less frequently, from a calorimeter shower or a product of a nuclear interaction in the detector.
	\item \textbf{Hadron punch-through}: Hits produced by particles different from muons; they are usually due to hadron shower remnants penetrating through the calorimeters and reaching the muon system.
	\item \textbf{Duplicate}: They are reconstructed muons having a $\chi^2$ smaller than the “best” muon present in the event; these duplicate candidates can often be caused by instrumental effects or imperfections in the pattern recognition algorithm of the reconstruction software.  
\end{itemize}

(per altri contributi vedi Vanff pag 97)\\
(from "Performance paper 7 TeV" :\\
the probability to misidentify a hadron as a muon is well below 1\%.

In order to optimize the muon reconstruction requirement to select only the muons produced in a particular kind of interaction, different identification (ID) categories have been developed and muons are assigned to them according to their characteristics. Some examples of muon ID types implemented in CMS are the following:
\begin{itemize}
	\item \textbf{Loose ID}: it aims to identify prompt muons originating at the PV and muons from light and heavy flavor decays. At the same time a low rate of the misidentification of charged hadrons as muons is maintained. To belong to this category a muon is required to be a muon selected by the PF algorithm and to be either a tracker or a global muon.
	\item \textbf{Medium ID}: it is a category optimized for prompt muons and muons from heavy flavor decay. It contains loose muons with a tracker track that uses hits from more than 80\% of the inner tracker layers it traverses. Moreover, the muon segment compatibility has to be greater than a certain threshold.
	\item \textbf{Tight ID}: it aims to suppress muons from decay in flight and from hadronic punch-through. A muon belonging to this category is a loose muon with a tracker track that uses hits from at least six layers of the inner tracker including at least one pixel hit. A tight muon has to be both a tracker and a global muon and the tracker muon must have segment matching in at least two of the muon stations, while the global fit to satisfy precise requirements on the $\chi^2$/ndf. Finally, some conditions on the impact parameter(defined as the distance of closest approach of the muon track with respect to the beamspot) must be fulfilled, in order to select only muons compatible with the PV. 
	\item \textbf{Soft ID}: it is a category optimized for low $\mathrm{p_{T}}$ muons ($<$10 GeV), containing tracker muons that satisfy high purity requirements and use hits from at least six layers of the inner tracker including at least one pixel hit. This kind of muons are loosely compatible with the PV.
	\item \textbf{High $\bm{\mathrm{p_{T}}}$ ID}: it is a category optimized for muons with a high $\mathrm{p_{T}}$ ($>$ 200 GeV). These muons are both tracker and global muons and fulfill the same requirements of the tight muons for the impact parameters, but they don’t have to satisfy any requirement on the $\chi^2$/ndf of the global fit. Moreover, these muons don’t have to be selected by the PF algorithm.
\end{itemize}

-	Plot contributi dei vari tipi di muoni ai muoni totali 
In Fig.\ref{fig:MuonContribution} the distributions of $\eta$ and $\phi$ of muons belonging to a sample of Soft muons (on the left) and Tight muons (on the right) are shown, both for data (points) and MC (histograms). The contribution from the different categories of muons to the total is shown with different colors.

\begin{figure}[h!]
   \centering
   \includegraphics[width=15cm]{MuonContribution}
	\caption{Distributions of kinematic variables for a sample of muons subdivided into the different categories, for data (points) and for simulation (histograms). The kinematic variables are the muon pseudorapidity (on the top), and azimuthal angle (bottom). For each variable, the left plot shows the distribution for Soft Muons, and the right plot that for Tight Muons \cite{ref69}.}
	\label{fig:MuonContribution} 
\end{figure}

\subsection{Muon Isolation}
[ Inserirlo e/o modificarlo con la def custom di quelli della Florida]
An important parameter used to distinguish between prompt muons and those from weak decays within jets is the so called \emph{Isolation} of the muon.
It is quantifies the number of particles in a cone with a chosen value of $\Delta R$ (defined in eq. \ref{eq:DeltaR}), having its axis corresponding to the direction of the muon  momentum.
In the analysis presented in this thesis a particular custom definition of the muon isolation has been developed in order to better distinguish the interesting muons from the background, based on consideration linked to the physics process of interest.
[ def. nuova isolation ]

\subsection{Muon efficiency}

\newpage
In Figg. \ref{fig:MuonEff_LooseID}, \ref{fig:MuonEff_MediumID}, \ref{fig:MuonEff_TightID} the muon efficiencies as a function of $\mathrm{p_{T}}$ and $\eta$, respectively for Loose ID, Medium ID and Tight ID are shown. The data used belong to the full 2017 data taking: they are collected in pp collisions at a center of mass energy of 13 TeV with single muon triggers and correspond to an integrated luminosity of 41.3 $fb^{-1}$.\\
The efficiencies are computed by means of a Tag and Probe method, exploiting the $Z\rightarrow\mu\mu$ resonance \cite{ref116}. For all the muon ID categories shown (loose, medium and tight), the number of all tracker tracks with $\mathrm{p_{T}}>$20 GeV has been used as denominator in the efficiency computation. No significant dependency with respect to the number of primary vertices was observed.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.45]{MuonEff_LooseID}
	\caption{Loose muon ID efficiency as a function of $\mathrm{p_{T}}$ and $\eta$ for 2017 data and MC \cite{ref70}.}
	\label{fig:MuonEff_LooseID}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.45]{MuonEff_MediumID}
	\caption{Medium muon ID efficiency as a function of $\mathrm{p_{T}}$ and $\eta$ for 2017 data and MC. The drops at around $\eta$ = 0.2 are due to the cracks between wheels in the muon detectors. The drops in the forward region are due to inactive chambers not modelled in the MC \cite{ref70}.}
	\label{fig:MuonEff_MediumID}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.45]{MuonEff_TightID}
	\caption{Tight muon ID efficiency as a function of $\mathrm{p_{T}}$ and $\eta$ for 2017 data and MC. The drops at around $\eta$ = 0.2 are due to the cracks between wheels in the muon detectors. The drops in the forward region are due to inactive chambers not modelled in the MC  \cite{ref70}.}
	\label{fig:MuonEff_TightID}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{dimuons_2016}
	\caption{The dimuon invariant mass distribution reconstructed by the CMS LHT with data  colected in 2016 data-taking at a center of mass energy of 13 TeV. The gray distribution correspond to the data collected with the inclusive double-muon trigger algorithm, while the colored ones have been collected with triggers dedicated to selecting resonances at low masses, as indicated in the legend.}
	\label{fig:dimuons_2016}
\end{figure}

-	Plot Efficienze delle varie ID [ + eventualmente pag.71-72 Raffaella and tag and probe pag 101 Vanohffer  ] \cite{ref70}


\section{Electron reconstruction}
The signature of an electron inside CMS is made of hits in the silicon detectors of the inner tracker and clusters inside the ECAL crystal, where the electron releases all its energy.
The measure of this latter energy deposit is the starting point for the electron’s reconstruction, which goes through the following steps: ( rendi i punti + esplicativi)
\begin{enumerate}
	\item Energy measurement
	\item Track seed selection / seeding
	\item Tracking
	\item Association track – cluster
\end{enumerate}

A standalone approach is combined with the global particle-flow (PF) algorithm for a better performance \cite{ref72}. 

\subsection{Energy measurement: clustering in ECAL}
Electrons interacting in the electromagnetic calorimeter deposit almost all their energy inside its crystals. However, due to the presence of the material in front of ECAL, it can happen that electrons can radiate photons and lose part of their energy via bremsstrahlung. It will result in an energy deposit in the calorimeter which is more contained and less spread over the crystals.
It is estimated that the electron energy lost before reaching the ECAL is on average $\sim$33\% to 86\% going from $\eta\ \approx$ 0 to $|\eta|\ \approx $ 1.4.
Therefore, estimating properly the value of this loss to add it to the energy deposited in the ECAL, is of crucial importance in electrons reconstruction \cite{ref71} \cite{ref73}. 
The photons radiated via bremsstrahlung are mainly spread in the $\phi$ direction (the spread in $\eta$ direction is usually negligible, except for electrons with very low $\mathrm{p_{T}}$ ( $<$5 GeV)) because of the bending of the electron trajectory due to the magnetic field.
In order to measure the energy of the radiated photons, two clustering algorithms have been developed: the “hybrid algorithm in the barrel and the “multi-5$\times$5” in the endcap.

The first algorithm, used in the ECAL barrel (EB), considers arrays of 5$\times$1 crystals in $\eta$-$\phi$ around the “seed crystal”, i.e. a crystal characterized by an excess in energy deposit with respect to the other regions (at least 1 GeV). If the total energy deposited in this group of crystals is greater than 0.1 GeV, they are grouped with the contiguous array of crystals, forming a cluster. The final global cluster, called SuperCluster (SC) have to contain a seed array with energy greater than a certain threshold.

The second algorithm, used in the ECAL endcaps (EE) and in the preshower (PS), is similar to the first one but considers a 5$\times$5 matrix around a seed crystal, whose energy has to exceed 1 GeV in order to be added to the SC.
In the end, the SC energy and position are measured: the former is given by the sum of all the energies of its clusters, while the second is calculated as the energy-weighted mean of the cluster positions. 
 
On the hand, being part of the particle flow reconstruction, there exist also another algorithm with the aim of reconstructing the particle showers individually.

\subsection{Track seed selection / seeding}
As seen in \ref{Track charged particles}, the tracks of charged particles inside the tracker can be reconstructed using the KF. However, in the case of electrons this reconstruction procedure ends up with a poor estimation of the track parameters, because these particles lose a large amount of their energy through bremsstrahlung in the tracker material, and a reduced hit-collection efficiency.
For these reasons, a dedicated tracking procedure has been developed for electrons \cite{ref76}.
 
The reconstruction of their track inside the silicon detectors starts with the generation of the track seeds from 2 or 3 hits in the pixel detector, combined with the positions of the vertices measured from the general charged particle tracks. To select the seeds, two complementary algorithms are used, and their results are combined at the end. These algorithms are the following:
\begin{itemize}
	\item \textbf{ECAL-driven seeding}: The SC energy and position are used to extrapolate the electron trajectory towards the inner layers of the tracker. If a reconstructed tracker seed that matches the prediction from the SC is found, it is selected.
	\item \textbf{Tracker-driven seeding}: The tracker tracks reconstructed using the Kalman filter algorithm are extrapolated towards the ECAL and matched to a SC.
\end{itemize}

The first algorithm is the best one for high $\mathrm{p_{T}}$ electrons, while the second is optimized from low $\mathrm{p_{T}}$ ones, when bremsstrahlung is negligible.

\subsection{Tracking}
The seeds selected in the previous step are used to build electron tracks: starting from them, a combinatorial track finding algorithm iteratively adds successive layers, taking into account their energy losses due to ionization and bremsstrahlung, modelled with the Bethe-Heitler (BH) formula \cite{ref74}.
Since the distribution of the energy loss in the BH model is non-Gaussian, the KF algorithm can no longer be used and it is substituted by a “Gaussian sum filter” (GSF) algorithm \cite{ref75}. The GSF models the energy loss distribution as a sum of six Gaussian distributions with different mean, width and amplitude.

\subsection{Association track - cluster}
Electrons candidates are finally reconstructed by associating a GSF track to a cluster in the ECAL, using not very restrictive criteria.
For the matching of ECAL-driven tracks are used the SC reconstructed using either the hybrid or the multi-5$\times$5 algorithm. 
For the Tracker-driven tracks whereas, a boosted decision tree (BDT) is used in combining the track observables and the SC observables to get a global identification variable.


\section{Jets reconstruction}
At LHC quarks and gluons are dominantly produced. However, due to the QCD confinement, they cannot be observed directly, but they fragment to a collimated bunch of hadrons flying roughly in the same direction, which is called “jet”. 
Their signature is an energy deposit in the calorimeters, along with a series of hits in the tracker, in case of charged hadrons. 

Jets are a background for several analysis, therefore is fundamental to reconstruct them properly \cite{ref77}!
For this purpose, an “anti-kt clustering algorithm” \cite{ref78} is used: 
Clustering algorithms classify calorimeter energy-depositions or reconstructed particles into jets. 
it takes the reconstructed particles as input and assigns them to a jet, if some criteria are met (??)

In the jet reconstruction the information from different subdetectors can be combined together in different ways, according to the particular characteristics of the jets that can therefore be divided in:
\begin{itemize}
	\item \textbf{Calorimeter jets}: they are reconstructed from energy deposits in the calorimeters (ECAL and HCAL) alone. 
	\item \textbf{Jet-plus-track (JPT) jets}: the tracker information is added to the calorimeters one to reconstruct the jets.
	\item \textbf{Particle Flow jets}: they are reconstructed taking as input the PF candidate particles and by clustering their four-momentum vectors. This greatly improves the jet momentum and spatial resolution with respect to calorimeter jets and therefore they are mostly used in CMS analysis.
\end{itemize}

[ There are different types of clustering algorithms. Cone algorithms use a top-down approach by finding coarse regions of energy flow in conical regions in ($\eta$-$\phi$) space, while sequential recombination algorithms use a bottom-up approach by combining particles starting from closest ones using a certain distance measure. One of such sequential recombination algorithms is the anti-kt algorithm, which is commonly used for jet clustering in CMS. 

Jets are commonly reconstructed in CMS using PF candidates and the anti-kt algorithm. 
distance parameter of R = 0.4 is used for $\sqrt{s}$ = 13 TeV proton-proton collisions. 

The "goodness" of this (last) solution is visible in Fig. \ref{fig:PF_Jets}, where a simulated dijet event is displayed and a comparison between the calorimeter jets and PF jets is made.
The reconstructed particles are closer in energy and direction to the jets of generated particles than the calorimeter jets !

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{PF_Jets}
	\caption{Jet reconstruction in a simulated dijet event.The particles clustered in the two PF jets are displayed with a thicker line. The PF jet $\mathrm{p_{T}}$ indicated as a radial line, is compared to the pT of the corresponding generated (Ref) and calorimeter (Calo) jets. The four-momentum of the jet is obtained by summing the four-momenta of its constituents \cite{ref60}.}
	\label{fig:PF_Jets}
\end{figure}

\newpage
\chapter{Analysis }

\section{Theorical intro}

Belle \cite{ref102}
BaBar \cite{ref103}
LHCb \cite{ref107}
ATLAS \cite{ref108}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{FD_SM}
	\caption{FD/SM.}
	\label{fig:FD_SM}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{FD_BSM}
	\caption{FD/BSM.}
	\label{fig:FD_BSM}
\end{figure}

\section{Preliminary considerations}
\subsection{Production of $\tau$ leptons at LHC}

The expected inclusive number of $\tau$ coming from different processes are reported in Tab. \ref{tab:ExpectedTau}. 

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|l|c|}
	\hline 
	$\mathrm{Process1}$		&	$\mathrm{Process2}$& 	$\mathrm{N.\ of\ \tau\ for\ L = ? }$\\ \hline \hline
	$pp\rightarrow c\bar{c}+...$	&	$D\rightarrow \tau\nu_{\tau}$	&	000\\
	 &	(95\% $D_{s}$, 5\% $D^{\pm}$)	&	\\  \hline
	\multirow{2}{*}{$pp\rightarrow b\bar{b}+...$}		&	$B\rightarrow \tau\nu_{\tau}+...$	&	000\\
	 &	(44\% $B^{\pm}$, 45\% $B^{0}$, 11\% $B_{s}^{0}$)	&	\\
	&	$B\rightarrow D(\tau\nu_{\tau})+...$	&	000\\ 
	 &	(98\% $D_{s}$, 2\% $D^{\pm}$)	&	\\ \hline \hline
	$pp\rightarrow W+...$	&	$W\rightarrow\tau\nu_{\tau}+...$	&	000\\
	$pp\rightarrow Z+...$	&	$Z+...\rightarrow\tau\tau+...$	&	000\\ 
	\hline
	\end{tabular}
	\caption{List of processes from which $\tau$ leptons are produced, with their corresponding expected inclusive number for L = ? // The charge-conjugated states are included. }
	\label{tab:ExpectedTau}
\end{table}



The relative $\tau$ yields are listed in Tab. \ref{tab:relativeTauYield}.

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|c|c|c|c|}
	\hline 
	$\mathrm{Mother\ meson}$ 	& 	$\mathrm{Quark\ composition}$	&	$\mathrm{Meson\ mass\ (GeV)}$	&	$\mathrm{Relative\ \tau\ yield}$\\ \hline \hline
	$D_{s}$	&	$c\bar{s}$	&	1.97	&	72\%\\
	$D^{+}$	&	$c\bar{d}$	&	1.87	&	3\%\\
	$B^{+}$	&	$\bar{b}u$	&	5.28	&	11\%\\
	$B^{0}$	&	$\bar{b}d$	&	5.28	&	11\%\\
	$B_{s}$		&	$\bar{b}s$	&	5.37	&	3\%\\ 
	\hline
	\end{tabular}
	\caption{List of mesons from which $\tau$ are produced with the relative tau yields. The charge-conjugated states are included. }
	\label{tab:relativeTauYield}
\end{table}


\subsection{Signal acceptance}

\section{Outline: search strategy}

\section{Data and MonteCarlo samples}
\subsection{Datasets}
For the analysis described in this thesis, I have used data collected by CMS in pp collisions at a center of mass energy $\sqrt{s}$ = 13 TeV during Run II in the whole 2017.\\ 
These datasets are listed in Tab. \ref{tab:2017datasets}.\\
They correspond to a total integrated luminosity: $\mathcal{L} = 41.49\ fb^{-1}$.\\

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|c|c|}
	\hline 
	$\mathrm{2017\ Datasets}$ 	& 	$\mathrm{Run\ range}$	&	$\mathrm{Integr.\ Luminosity\ (fb^{-1})}$\\ \hline \hline
	/DoubleMuonLowMass/Run2017B-17Nov2017-v1/AOD	&	000							&	4.79\\
	/DoubleMuonLowMass/Run2017C-17Nov2017-v1/AOD	&	000							&	9.63\\
	/DoubleMuonLowMass/Run2017D-17Nov2017-v1/AOD	&	000							&	4.25\\
	/DoubleMuonLowMass/Run2017E-17Nov2017-v1/AOD	&	000							&	9.3\\
	/DoubleMuonLowMass/Run2017F-17Nov2017-v1/AOD	&	000							&	13.52\\ \hline
	Whole 2017 data															&	000							&	41.49\\
	\hline
	\end{tabular}
	\caption{Datasets of the whole 2017 data-taking that have been used for the analysis described in this thesis.}
	\label{tab:2017datasets}
\end{table}

The json file used for data processing is:\\
$\mathrm{Collisions17/13TeV/ReReco/Cert_294927-306462_13TeV_EOY2017ReReco_Collisions17_JSON_v1.txt}$.

\subsection{MonteCarlo samples}

The MC samples used in this analysis are listed in Tab. \ref{tab:MCsamples}.

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|c|c|}
	\hline
	$\mathrm{Simulated\ process}$ 	& 	$\mathrm{MC\ Dataset\ name}$	&	$\mathrm{N. events\ (M)}$\\ \hline \hline
	$\mathrm{D_{s}\rightarrow\tau \nu_{\tau}\rightarrow 3\mu\ \nu_{\tau}}$	&	000	&	4.79\\
	$\mathrm{B^{0}\rightarrow\tau \nu_{\tau}\rightarrow 3\mu\ \nu_{\tau}}$	&	000	&	9.63\\
	$\mathrm{B^{\pm}\rightarrow\tau \nu_{\tau}\rightarrow 3\mu\ \nu_{\tau}}$	&	000	&	4.25\\
	$\mathrm{D_{s}\rightarrow\phi \pi \rightarrow 2\mu\ \pi}$	&	000	&	9.3\\
	\hline
	\end{tabular}
	\caption{MC samples used for the analysis described in this thesis.}
	\label{tab:MCsamples}
\end{table}


The branching fractions of the B and D mesons decays are listed in Tab. \ref{tab:MesonsBR}.
\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|r|c|}
	\hline
	$\mathrm{Process}$ 	& 	$\mathrm{Branching\ ratio (BR)}$	&	$\mathrm{Reference}$\\ \hline \hline
	$\mathrm{D_{s}\rightarrow\tau \nu_{\tau}}$	&	5.48 $\pm$ 0.23\%	&	PDG \cite{ref112}\\
	$\mathrm{B^{+}\rightarrow\tau \nu_{\tau} D_{0}^{*}}$	&	2.7 $\pm$ 0.3\%	&	PDG \cite{ref112}\\
	$\mathrm{other B^{+}\rightarrow\tau \nu_{\tau} X}$	&	0.7\%	&	PYTHIA \cite{ref104}\\
	$\mathrm{B^{0}\rightarrow\tau \nu_{\tau} D^{+*}}$	&	2.7 $\pm$ 0.3\%	&	PDG \cite{ref112}\\
	$\mathrm{other B^{0}\rightarrow\tau \nu_{\tau} X}$	&	0.7\%	&	PYTHIA \cite{ref104}\\
	$\mathrm{B^{+}\rightarrow D_{s} X}$	&	9.0 $\pm$ 1.5\%		&	PDG \cite{ref112}\\
	$\mathrm{B^{0}\rightarrow D_{s} X}$		&	10.3 $\pm$ 2.1\%		&	PDG \cite{ref112}\\
	$\mathrm{D_{s}\rightarrow \phi(\mu\mu)\pi}$	&	1.3($\pm$ 0.1 $\cdot 10^{-5}$)	&	PDG \cite{ref112}\\
	\hline
	\end{tabular}
	\caption{MC samples used for the analysis described in this thesis.}
	\label{tab:MesonsBR}
\end{table}

\section{Event selection}

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|cc|c|}
	\hline
		 	& 	\multicolumn{2}{c|}{Signal}	&	Data\\
		 	&	$D_{s}\rightarrow\tau\nu_{\tau}$	&	$B^{\pm}/B^{0}\rightarrow\tau...$	&	 \\ \hline 	
	Produced in pp collisions	&	000	&	000	&	 \\
		(with three muons in fiducial volume)	&	000	&	000	&	 \\
	L1/HLT trigger	&	000	&	000	&	  \\
	At least 3 GlobalMuon ($\mathrm{p_{T}} >$ 2 GeV)	&	000	&	000	&	 000\\
	Trimuon candidate selection	&	000	&	000	&	 000\\
	\hline
	\end{tabular}
	\caption{Bla Bla}
	\label{tab:NEvents}
\end{table}

\section{Signal Normalization}
\subsection{Events triggered by DoubleMu L1 seeds}
\begin{equation}
\mathrm{N_{sig(D)}} = \mathcal{L}\ \sigma(pp \rightarrow D_{s})\ \mathcal{B}(D_{s} \rightarrow \tau \nu_{\tau})\ \mathcal{B}(\tau \rightarrow 3\mu)\ \mathcal{A}_{3\mu(D)}\ \epsilon^{3\mu}_{reco}\ \epsilon^{2\mu}_{trig}
\end{equation}

\begin{equation}
\mathrm{N} = \mathcal{L}\ \sigma(pp \rightarrow D_{s})\ \mathcal{B}(D_{s} \rightarrow \phi \pi \rightarrow \mu \mu \pi)\ \mathcal{A}_{2\mu\pi}\ \epsilon^{2\mu\pi}_{reco}\ \epsilon^{2\mu}_{trig}
\end{equation}

\begin{equation}
\mathrm{N_{sig(D)}} = N\ \cdot\frac{\mathcal{B}(D_{s} \rightarrow \tau \nu_{\tau})}{\mathcal{B}(D_{s} \rightarrow \phi \pi \rightarrow \mu \mu \pi)}\ \frac{\mathcal{A}_{3\mu(D)}}{\mathcal{A}_{2\mu\pi}}\ \frac{\epsilon^{3\mu}_{reco}}{\epsilon^{2\mu\pi}_{reco}}\ \mathcal{B}(\tau \rightarrow 3\mu)
\end{equation}

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|c|c|c|}
	\hline
	$\mathrm{ Run}$ 	& 	$\mathrm{Fitted\ D_{s}\rightarrow\phi(\mu\mu)\pi\ yields\ (fb^{-1})}$	&	$\mathrm{Data/MC\ ratio}$\\ \hline 
	2017 B	&	000	&	000\\
	2017 C	&	000	&	000\\
	2017 D	&	000	&	000\\
	2017 E	&	000	&	000\\
	2017 F	&	000	&	000\\ \hline
	Whole 2017	&	000	&	000\\
	\hline
	\end{tabular}
	\caption{Bla Bla}
	\label{tab:N_Ds}
\end{table}

\subsection{B/D ratio}

\begin{equation}
\mathrm{N_{sig(B)}} = \mathcal{L}\ \sigma(pp \rightarrow B)\ \mathcal{B}(B \rightarrow \tau+...)\ \mathcal{B}(\tau \rightarrow 3\mu)\ \mathcal{A}_{3\mu(B)}\ \epsilon^{3\mu}_{reco}\ \epsilon^{2\mu}_{trig}
\end{equation}

\begin{equation}
f\ =\ \frac{\sigma(pp \rightarrow B)\ \mathcal{B}(B \rightarrow D_{s}+...)}{\sigma(pp \rightarrow D_{s})}
\end{equation}

\begin{equation}
\mathrm{N_{sig(B)}} = N \cdot\ f \cdot\ \frac{\mathcal{B}(B \rightarrow \tau+...)}{\mathcal{B}(D_{s} \rightarrow \phi \pi \rightarrow \mu \mu \pi)\ \mathcal{B}(B \rightarrow D_{s}+...)}\ \frac{\mathcal{A}_{3\mu(B)}}{\mathcal{A}_{2\mu\pi}}\ \frac{\epsilon^{3\mu}_{reco}}{\epsilon^{2\mu\pi}_{reco}}\ \mathcal{B}(\tau \rightarrow 3\mu)
\end{equation}

\subsection{Control channel} 
MC validation with $D_{s}\rightarrow\phi(\mu\mu)\pi$.

\section{Multivariate Analysis (MVA)}
\subsection{Introduction to MVA: why to use it?}
The Multivariate analysis has assumed progressively a central role in high energy physics searches \cite{ref117b}. In this field, this kind of analysis aims to exploit as much information as possible from the characteristics of each event in order to distinguish between event types (eg. signal and background).\\
In order to understand how this analysis works and which are the improvements in using it with respect to "traditional" analysis performed by successive cuts, let's look at the scatter plots in Fig. \ref{fig:MVA_examples}. They show the distribution of two variables $x_{1}$ and $x_{2}$ which represent two out of a potentially large number of quantities measured for each event, with different decision boundaries (cuts, lineary boundary, non linear boundary). The signal events are indicated with blue circles while the red triangles represent the background ones. \\

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{MVA_examples}
	\caption{Scatter plots of two variables corresponding to two hypotheses: signal (blue) and background (red). Event selection could be based, e.g., on (a) cuts, (b) a linear boundary, (c) a nonlinear boundary.
  \cite{ref117b}.}
	\label{fig:MVA_examples}
\end{figure}

It is evident that rectangular and diagonal cuts are not as good as nonlinear boundaries in classifying the events. In general, the best decision boundary is a surface in the n-dimensional space of input variables, which can be represented by an equation of the form $y(\bm{x}) = y_{cut}$, where $y_{cut}$ is some constant. Events are classified as signal if they are on one side of the boundary, e.g., $y(\bm{x}) \leq y_{cut}$, could represent the acceptance region and $y(\bm{x}) > y_{cut}$ could be the rejection region.\\
- oppure utilizzo come "scalar test" statistics (vedi 14-15 Cowan per more details )

\subsection{Boosted Decision Tree (BDT)}
\subsubsection*{Decision Tree}
A decision tree is a binary tree structured classifier, defined by a collection of successive cuts on the set of input variables. It is schematically represented in Fig. \ref{fig:DecisionTree}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{DecisionTree}
	\caption{Scheme of a decision tree. A sequence of binary splits using the discriminating variables xi is applied to the data, starting from the root node. Each split uses the variable that, at that node, discriminates in the best way signal(S) and background(B). The leaf nodes at the bottom end of the tree are labeled (S or B) depending on the majority of events that end up in the respective nodes \cite{ref117}.}
	\label{fig:DecisionTree}
\end{figure}

Starting from the entire sample of training events in the root node, out of all of the possible input variables, one finds the ones that provide the best separation between signal and background by use of a single cut. Then, repeated left/right (yes/no) decisions are taken on one single variable at a time until a stop criterion is fulfilled. \\
In this way, the phase space is split into many regions that are eventually classified as signal or background, depending on the majority of training events that end up in the final leaf node. \\
Therefore, the difference of this classifier, with respect to the case of rectangular cuts, is that whereas a cut-based analysis is able to select only one hypercube as region of phase space, the decision tree is able to split the phase space into a large number of hypercubes, each of which is identified as either “signal-like” or “background-like”.
For classification trees, the path down the tree to each leaf node represents an individual cut sequence that selects signal or background depending on the type of the leaf node.

A shortcoming of decision trees is their instability with respect to statistical fluctuations in the training sample from which the tree structure is derived.\\
This problem can be overcome by using a Boosted Decision Tree.

\subsubsection*{Boosting the tree...}
The boosting of a decision tree extends this concept from one tree to several trees, which form a \emph{forest}. \\
The trees are derived from the same training ensemble by reweighting events (procedure called “boosting"), and are finally combined into a single classifier which is given by an average of the individual decision trees.
This boosting increases the statistical stability of the classifier and is able to drastically improve the separation performance compared to a single decision tree. \\
(In many cases, the boosting performs best if applied to trees that, taken individually, have not much classification power. These so called “weak classifiers” are small trees, limited in growth to a typical tree depth of as small as two, depending on the how much interaction there is between the different input variables. By limiting the tree depth during the tree building process (training), the tendency of overtraining for simple decision trees which are typically grown to a large depth and then pruned, is almost completely eliminated.)\\
A number of boosting algorithms have been developed, and these differ primarily in the rule used to update the weights; the one employed in the BDT used in this analysis is the so-called \emph{AdaBoost} (adaptive boost), which is explained in detail in \cite{ref117a}.

%\subsubsection*{Adaptive Boost (AdaBoost)}  
%In a classification problem, events that were misclassified during the training of a decision tree are given a higher event weight in the training of the following tree. 
%Starting with the original event weights when training the first decision tree, the subsequent tree is trained using a modified event sample where the weights of previously misclassified events are multiplied by a common boost weight $\alpha$. The boost weight is derived from the misclassification rate, err, of the previous tree (By construction, the error rate is err $\leq$ 0.5 as the same training events used to classify the output nodes of the previous tree are used for the calculation of the error rate.)
%
%\begin{equation}
%\alpha = \frac{1-err}{err}
%\end{equation}
%
%The weights of the entire event sample are then renormalised such that the sum of weights remains constant.
%We define the result of an individual classifier as $h(\bm{x})$, with ($\bm{x}$ being the tuple of input variables) encoded for signal and background as $h(\bm{x})$ = +1 and -1, respectively.\\ 
%The boosted event classification $y_{Boost}(\bm{x})$ is then given by
% 
%\begin{equation}
%y_{Boost}(\bm{x}) = \frac{1}{N_{collection}}\cdot\sum_{i}^{N_{collection}}\ ln(\alpha_{i})\cdot h_{i} (\bm{x})
%\end{equation}
%
% where the sum is over all classifiers in the collection. Small (large) values for yBoost(x) indicate a background-like (signal-like) event. Equation (the previous one) represents the standard boosting algorithm.
%AdaBoost performs best on weak classifiers, meaning small indivdidual decision trees with a tree depth of often as small 2 or 3, that have very little discrimination power by themselves. Given such small trees, they are much less prone to overtraining compared to simple decision trees and as an ensemble outperform them typically by far. The performance is often further enhanced by forcing a “slow learing” and allowing a larger number of boost steps instead. The learning rate of the AdaBoost algorithm is controled by a parameter $\beta$ giving as an exponent to the boost weight $\alpha\rightarrow\alpha^{\beta}$, which can be modified using the configuration option string of the MVA method to be boosted.

\subsubsection{Training a decision tree}
(ridurre: in molte parti è una ripetizione di quanto già detto! Accorpare al precedente!)

The training of a decision tree is the process that defines the splitting criteria for each node. It starts with the root node, where an initial splitting criterion for the full training sample is determined. The split results in two subsets of training events that each go through the same algorithm of determining the next splitting iteration. This procedure is repeated until the whole tree is built. At each node, the split is determined by finding the variable and corresponding cut value that provides the best separation between signal and background. The node splitting stops once it has reached the minimum number of events which is specified in the BDT configuration (option nEventsMin). The leaf nodes are classified as signal or background according to the class the majority of events belongs to. 
% If UseYesNoLeaf is set to false the end-nodes are classified according to their purity. (The purity of a node is given by the ratio of signal events to all events in that node. Hence pure background nodes have zero purity.)
A variety of separation criteria can be configured to assess the performance of a variable and a specific cut requirement. Because a cut that selects predominantly background is as valuable as one that selects signal, the criteria are symmetric with respect to the event classes. All separation criteria have a maximum where the samples are fully mixed, i.e., at purity p = 0.5, and fall off to zero when the sample consists of one event class only. 
%Tests have revealed no significant performance disparity between the following separation criteria:
%\begin{itemize}
%	\item Gini Index(default), defined by $p \cdot (1-p)$;
%	\item Cross entropy, defined by $-p\cdot\ ln\ (p)-(1-p)\cdot ln\ (1-p)$;
%	\item Misclassification error, defined by 1 - max(p, 1 - p);
%	\item Statistical significance, defined by S/$\sqrt{S + B}$
%	\item Average squared error, defined by $1/N \cdot \prod_{n}(y-\hat{y})^2$ for regression trees where y is the regression target of each event in the node and $\hat{y}$ is its mean value over all events in the node (which would be the estimate of y that is given by the node).
%\end{itemize}

Since the splitting criterion is always a cut on a single variable, the training procedure selects the variable and cut value that optimises the increase in the separation index between the parent node and the sum of the indices of the two daughter nodes, weighted by their relative fraction of events. The cut values are optimised by scanning over the variable range with a granularity that is set via the option nCuts. 

PROVARE!!! The default value of nCuts=20 proved to be a good compromise between computing time and step size. Finer stepping values did not increase noticeably the performance of the BDTs. However, a truly optimal cut, given the training sample, is determined by setting nCuts=-1. This invokes an algorithm that tests all possible cuts on the training sample and finds the best one. 

\subsubsection*{Variable ranking}
A ranking of the BDT input variables is derived by counting how often the variables are used to split decision tree nodes, and by weighting each split occurrence by the separation gain-squared it has achieved and by the number of events in the node.\\ 
This measure of the variable importance can be used for a single decision tree as well as for a forest.

\subsubsection*{Performance}
PUNTO a FAVORE: Decision trees are also insensitive to the inclusion of poorly discriminating input variables (which is not the case of neural networks, for example). 

Boosted decision trees have become increasingly popular in particle physics in recent years. One of their advantages is that they are relatively insensitive to the number of input variables used in the data vector x. Components that provide little or no separation between signal and background are rarely chosen as for the cut that provides separation, i.e., to split the tree, and thus they are effectively ignored. Decision trees have no difficulty in dealing with different types of data; these can be real, integer, or they can simply be labels for which there is no natural ordering (categorical data). Furthermore, boosted decision trees are surprisingly insensitive to overtraining. That is, although the error rate on the test sample will not decrease to zero as one increases the number of boosting iterations (as is the case for the training sample), it tends not to increase. Further discussion of this point can be found in \cite{ref117c}.
-- copia-e-incolla- END

- TMVA\\
The Toolkit for Multivariate Analysis (TMVA) provides a ROOT-integrated \cite{ref117} environment for the application of multivariate classification techniques.\\
All TMVA techniques belong to the family of \emph{supervised learning} algorithms. They make use of training events, for which the desired output is known, to determine the mapping function that describes a decision boundary (classification). This function can contain various degrees of approximations and may be a single global function, or a set of local models. \\
A typical TMVA classification consists of two independent phases: the \emph{training phase}, where the multivariate methods are trained, tested and evaluated, and an \emph{application phase}, where the chosen methods are applied to the concrete classification or regression problem they have been trained for.
 

\subsection{BDT training}
\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|c|}
	\hline
	Input variable	&	Importance	\\ \hline
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	var	&	000\\
	\hline
	\end{tabular}
	\caption{Bla Bla}
	\label{tab:BDT_ranking}
\end{table}

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|c|}
	\hline
	Parameter		&	Setting (default)	\\ \hline
	NTrees	&	800	\\
	nCuts	&	20	\\
	MaxDepth		&	3	\\
	BoostType	&	AdaBoost	\\
	\hline
	\end{tabular}
	\caption{Bla Bla}
	\label{tab:BDT_parameters}
\end{table}

\subsection{Event categorization}

\section{Evaluation of systematics}
Bla Bla \cite{ref118}

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|l|c|c|}
	\hline
	Source of uncertainty	& 	Yield		&	Shape\\ \hline 
	Uncertainty on $D_{s}$ normalization [ 3.5\% ]	&	000	&	 \\
	Uncertainty on measuring f (B/D ratio) [ 10\% ]	&	000	&	 \\
	Uncertainty on n. of events triggered by trimuon trigger [ 12\% ]	&	000	&	 \\
	Relative uncertainty in $\mathcal{B}(D_{s} \rightarrow\phi\pi\rightarrow\mu\mu\pi$) [ 8\% ]	&	000	&	 \\
	Relative uncertainty in $\mathcal{B}(D_{s}\rightarrow\mu\nu)$ [ 4\% ]	&	000	&	 \\
	Relative uncertainty in $\mathcal{B}(B\rightarrow D_{s} + ...)$ [ 16\% ]		&	000	&	 \\
	Relative uncertainty in $\mathcal{B}(B \rightarrow \tau + ...)$ [ 11\% ]	&	000	&	 \\
	Uncertainty on the ratio of acceptances $\mathcal{A}_{sig}\ /\ \mathcal{A}_{2\mu\pi}$ [ 1\% ]	&	000	&	 \\
	Muon reconstruction efficiency [ 1.5\% ]	&	000	&	 \\
	BDT cut efficiency [ 5\% ]	&	000	&	 \\
	Muon momentum scale uncertainty [ 0.2\% ]	&	-	&	yes \\
	Muon momentum resolution uncertainty [ 10\% ]	&	-	&	 yes\\
	\hline
	\end{tabular}
	\caption{Bla Bla}
	\label{tab:Uncertainties}
\end{table}

\section{Results}

\begin{equation}
q_{\mu} = -\mathrm{2\ ln}\ \frac{\mathcal{L}\ (\mathrm{obs|\ \mu\cdot s+b,\ \hat{\theta_{\mu}}})}{\mathcal{L}\ (\mathrm{obs|\ \hat{\mu}\cdot s+b,\ \hat{\theta}})}
\end{equation}

\begin{equation}
\mathcal{L}\mathrm{(data|\ \mu s+b)}\ \sim\ e^{-(\mu S+B)}\ \prod_{i}\mathcal{P}\mathrm{(x_{i}|\ \mu s+b)}
\end{equation}

\begin{equation}
\mathrm{CL_{s}\ =\ \frac{P\ (q_{\mu}\geq q_{\mu}^{obs}\ |\mu\cdot\ s+b)}{P\ (q_{\mu}\geq q_{\mu}^{obs}\ |\ b)}} \leq 0.1
\end{equation}

\begin{table}[h!]\footnotesize
	\centering
	\begin{tabular}{|c|cc|cc|}
	\hline
		 	& 	\multicolumn{2}{c|}{Signal}		&	\multicolumn{2}{c|}{Background}\\
		 	&	sub-category 1	&	sub-category 2	&	sub-category 1	&	sub-category 2\\ \hline 	
	Category A	&	000	&	000	&	000	&	000\\
	Category B	&	000	&	000	&	000	&	000\\
	Category C	&	000	&	000	&	000	&	000\\
	\hline
	\end{tabular}
	\caption{Bla Bla}
	\label{tab:Results}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\chapter*{Conclusions}


\begin{thebibliography}{90}             
%\addcontentsline{toc}{chapter}{Bibliografia}

% Ref cap1
	\bibitem{ref0} ROOT, \url{http://root.cern.ch}.
	% Higgs discovery
	\bibitem{ref1} ATLAS Collaboration, \emph{“Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC”}, Phys. Lett. B 716 (2012) 1, \href{https://www.sciencedirect.com/science/article/pii/S037026931200857X}{doi:10.1016/j.physletb.2012.08.020}, \href{https://arxiv.org/abs/1207.7214}{arXiv:1207.7214}.
	
	\bibitem{ref2} CMS Collaboration, \emph{“Observation of a new boson at a mass of 125 GeV with the CMS experiment at the LHC”}, Phys. Lett. B 716 (2012) 30, \href{https://www.sciencedirect.com/science/article/pii/S0370269312008581}{doi:10.1016/j.physletb.2012.08.021}, \href{https://arxiv.org/abs/1207.7235}{arXiv:1207.7235}.
	
	\bibitem{ref3} CMS Collaboration,\emph{“Observation of a new boson with mass near 125 GeV in pp collisions at $\sqrt{s}$ = 7 and 8 TeV”}, JHEP 1306 (2013) 081, \href{https://link.springer.com/article/10.1007/JHEP06(2013)081}{doi:10.1007/JHEP06(2013)081}, \href{https://arxiv.org/abs/1303.4571}{arXiv:1303.4571}.

	
	\bibitem{ref4}
	\bibitem{ref5}
	\bibitem{ref6}
	\bibitem{ref7}
	\bibitem{ref8}
	\bibitem{ref9}
	\bibitem{ref10}
	\bibitem{ref11}
	\bibitem{ref12}
	\bibitem{ref13}
	\bibitem{ref14}
	\bibitem{ref15}
	\bibitem{ref16}
	\bibitem{ref17}
	\bibitem{ref18}
	\bibitem{ref19}
	\bibitem{ref20}
	
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
% Ref cap2
	% LHC
	\bibitem{ref30} L. Evans and P. Bryant, \emph{“LHC Machine”}, JINST 3 (2008) S08001, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08001/meta}{doi:10.1088/1748- 0221/3/08/S08001}.
	% LEP
	\bibitem{ref31} \emph{“The Large Electron-Positron Collider”}, \url{http://cds.cern.ch/record/1997351, July 2012}.
	% CMS
	\bibitem{ref32} CMS Collaboration, \emph{”The CMS experiment at the CERN LHC”}, JINST 3 (2008) S08004, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08004}{doi:10.1088/1748-0221/3/08/S08004}.
	
	\bibitem{ref33} CMS Collaboration, \emph{”CMS Detector Performance and Software: Technical Design Report”}, 2006. CERN/LHCC 2006-001, CMS TDR 8.1, \url{https://cdsweb.cern.ch/record/922757/files/lhcc-2006-001.pdf}.
	% ATLAS
	\bibitem{ref34} ATLAS Collaboration, \emph{“The ATLAS Experiment at the CERN Large Hadron Collider”}, JINST 3 (2008) S08003, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08003/meta}{doi:10.1088/1748-0221/3/08/S08003}.
	% ALICE
	\bibitem{ref35} ALICE Collaboration, \emph{“The ALICE experiment at the CERN LHC”}, JINST 3 (2008) S08002, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08002/meta}{doi:10.1088/1748-0221/3/08/S08002}.
	% LHCb
	\bibitem{ref36} LHCb Collaboration, \emph{“The LHCb Detector at the LHC”}, JINST 3 (2008) S08005, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08005/meta}{doi:10.1088/1748-0221/3/08/S08005}.
	% LUMI public results
	\bibitem{ref37} CMS Collaboration, \emph{“CMS Luminosity - Public Results”}, \url{https://twiki.cern.ch/twiki/bin/view/CMSPublic/LumiPublicResults}.
	
	% Magnet
	\bibitem{ref44} CMS Collaboration, \emph{“The CMS magnet project: Technical Design Report”},1997. CERN/LHCC 97-010, CMS TDR 1, \url{http://cds.cern.ch/record/331056?ln=it}.
	
	\bibitem{ref38} CMS Collaboration, \emph{“Precise Mapping of the Magnetic Field in the CMS Barrel Yoke using Cosmic Rays”}, JINST 5 (2010) T03021, \href{https://iopscience.iop.org/article/10.1088/1748-0221/5/03/T03021/meta}{doi:10.1088/1748- 0221/5/03/T03021}.
	%Tracker
	\bibitem{ref39} CMS Collaboration, \emph{“The CMS tracker system project: Technical Design Report”},1998. CERN/LHCC 98-006, CMS TDR 5, \url{http://cdsweb.cern.ch/record/368412}.
	
	\bibitem{ref40} CMS Collaboration, \emph{“The CMS tracker: addendum to the Technical Design Report"},2000. CERN/LHCC 2000-016, CMS TDR 5.1, \url{http://cdsweb.cern.ch/record/490194}.
	
	\bibitem{ref41} CMS Collaboration, \emph{“CMS Tracking Performance Results from early LHC Operation”}, Eur. Phys. J. C 70 (2010) 1165, \href{https://link.springer.com/article/10.1140/epjc/s10052-010-1491-3}{doi:10.1140/epjc/s10052- 010-1491-3}, \href{https://arxiv.org/abs/1007.1988}{arXiv:1007.1988}.
	% Calo
	\bibitem{ref42} CMS Collaboration, \emph{“The Electromagnetic Calorimeter Project: Technical Design Report”}, 1997. CERN/LHCC 97-33, CMS TDR 4, \url{http://cds.cern.ch/record/349375?ln=it}.
	
	\bibitem{ref43} CMS Collaboration, \emph{“The hadron calorimeter project: technical design report”}, 1997. CERN/LHCC 97-031, CMS TDR 2, \url{http://cdsweb.cern.ch/record/357153}.
	% Muon system
	\bibitem{ref45} CMS Collaboration, \emph{“The muon project: Technical Design Report”},1997. CERN/LHCC 97-032, CMS TDR 3, \url{http://cdsweb.cern.ch/record/343814}.	
	
	\bibitem{ref46} CMS Collaboration, \emph{“Performance of the CMS muon detector and muon reconstruction with proton-proton collisions at $\sqrt{s}$ = 13 TeV"}, JINST 13 (2018) P06015, \href{https://iopscience.iop.org/article/10.1088/1748-0221/13/06/P06015}{doi:10.1088/1748-0221/13/06/P06015}, \href{https://arxiv.org/abs/1804.04528}{arXiv:1804.04528}.
	
	\bibitem{ref47} CMS Collaboration, \emph{“Performance of the CMS Muon Detectors in 2016 collision runs”}, CERN-CMS-DP-2016-046, \url{https://cds.cern.ch/record/2202964}.
	% Trigger
	\bibitem{ref48} CMS Collaboration, \emph{“CMS TriDAS project: Technical Design Report, Volume 1: The Trigger Systems”},2000. CERN/LHCC 2000-038, CMS TDR 6.1, \url{http://cds.cern.ch/record/706847?ln=it}.
	
	\bibitem{ref49} CMS Collaboration, \emph{“CMS The TriDAS Project: Technical Design Report, Volume 2: Data Acquisition and High-Level Trigger”},2002. CERN/LHCC 2002-026, CMS TDR 6, \url{http://cds.cern.ch/record/578006?ln=it}.
	
	\bibitem{ref50} CMS Collaboration, \emph{“CMS computing: Technical Design Report”},2005. CERN/LHCC 2005-023, CMS TDR 7, \url{http://cds.cern.ch/record/838359?ln=it}.
	
	\bibitem{ref51}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
% Ref cap3
	% PF
	\bibitem{ref60} CMS Collaboration, \emph{“Particle-flow reconstruction and global event description with the CMS detector”}, JINST 12 (2017) P10003, \href{https://iopscience.iop.org/article/10.1088/1748-0221/12/10/P10003/pdf}{doi:10.1088/1748- 0221/12/10/P10003}, \href{https://arxiv.org/abs/1706.04965}{arXiv:1706.04965}.
	% MET
	\bibitem{ref61} CMS Collaboration,\emph{“Missing transverse energy performance of the CMS detector”}, JINST 16 (2011) P09001, \href{https://iopscience.iop.org/article/10.1088/1748-0221/6/09/P09001}{doi 10.1088/1748-0221/6/09/P09001}, \href{https://arxiv.org/abs/1106.5048}{arXiv:1106.5048}.
	% Tracker
	\bibitem{ref62} CMS Collaboration, \emph{“Description and performance of track and primary-vertex reconstruction with the CMS tracker”}, JINST 9 (2014) P10009, \href{https://iopscience.iop.org/article/10.1088/1748-0221/9/10/P10009/meta}{doi:10.1088/1748-0221/9/10/P10009}, \href{https://arxiv.org/abs/1405.6569}{arXiv:1405.6569}.
	
	\bibitem{ref62a}  CMS Collaboration, \href{https://twiki.cern.ch/twiki/bin/view/CMSPublic/TrackingPOGResults2017}{CMS Tracking POG Performance Plots for 2017 Dataset}.	
	
	% Kalman Filter
	\bibitem{ref66} Kalman, R. E., \emph{“A New Approach to Linear Filtering and Prediction Problems”}, Journal of Basic Engineering, vol. 82, p. 35, 1960, \href{http://www.unitedthc.com/DSP/Kalman1960.pdf}{doi:10.1115/1.3662552}.
	
	\bibitem{ref63} P. Billoir, \emph{“Progressive track recognition with a Kalman like fitting procedure”}, Comput. Phys. Commun. 57 (1989) 390, \href{https://www.sciencedirect.com/science/article/pii/001046558990249X}{doi: 10.1016/0010-4655(89)90249-X}.
	
	\bibitem{ref64} P. Billoir and S. Qian, \emph{“Simultaneous pattern recognition and track fitting by the Kalman filtering method”}, Nucl. Instrum. Meth. A 294 (1990) 219, \href{https://www.sciencedirect.com/science/article/pii/016890029091835Y}{doi:10.1016/0168-9002(90)91835-Y}.
	
	\bibitem{ref65} R. Fruhwirth, \emph{“Application of Kalman filtering to track and vertex fitting”}, Nucl. Instrum. Meth. A 262 (1987) 444, \href{https://www.sciencedirect.com/science/article/pii/0168900287908874}{doi:10.1016/0168-9002(87)90887-4}.
	% Vertex
	\bibitem{ref68} R. Fruhwirth, W. Waltenberger and P. Vanlaer, \emph{“Adaptive Vertex Fitting”}, J. Phys. G: Nucl. Part. Phys. 34 N343 (2007), \href{https://iopscience.iop.org/article/10.1088/0954-3899/34/12/N01}{doi.org/10.1088/0954-3899/34/12/N01}.
	% Muons
	% vedi ref 46
	\bibitem{ref69} CMS Collaboration, \emph{“Performance of CMS muon reconstruction in pp collision events at $\sqrt{s}$ = 7 TeV”}, JINST 7 (2012) P10002, \href{https://iopscience.iop.org/article/10.1088/1748-0221/7/10/P10002/meta}{doi 10.1088/1748- 0221/7/10/P10002}, \href{https://arxiv.org/abs/1206.4071}{arXiv 1206.4071}.
	
	\bibitem{ref70} CMS Collaboration, \href{https://cds.cern.ch/record/2629364/files/DP2018_042.pdf}{Muon identification and isolation efficiencies with 2017 and 2018 data}.
	
	% Electrons
	\bibitem{ref72} S. Baffioni et al., \emph{“Electron reconstruction in CMS”}, Eur.Phys.J., vol. C49, p. 1099, 2007, \href{https://link.springer.com/article/10.1140\%2Fepjc\%2Fs10052-006-0175-5}{doi:10.1140/epjc/s10052-006-0175-5}.
		
	\bibitem{ref71} CMS Collaboration, \emph{“Performance of electron reconstruction and selection with the CMS detector in proton-proton collisions at sqrt(s) =8 TeV}”, JINST 10 (2015) no.06, P06005, \href{https://arxiv.org/abs/1502.02701}{arXiv:1502.02701}.
	
	\bibitem{ref73} Khachatryan, Vardan and others, \emph{“Electron and photon performance in CMS with the full 2016 data sample”}, CMS-DP-2017-004, CERN-CMS-DP-2017-004, \url{https://cds.cern.ch/record/2255497/?ln=it}.
	
	 \bibitem{ref74} H. Bethe and W. Heitler, \emph{“On the Stopping of fast particles and on the creation of positive electrons”}, Proc.Roy.Soc.Lond., vol. A146, p. 83, 1934, \href{https://doi.org/10.1098/rspa.1934.0140}{doi:10.1098/rspa.1934.0140}.
	
	\bibitem{ref75} W.Adam, R.Fruhwirth, A.Strandlie, and T.Todorov, \emph{“Reconstruction of electrons with the Gaussian-sum filter in the CMS tracker at LHC”}, J. Phys. G 31 (2005) N9, \href{https://iopscience.iop.org/article/10.1088/0954-3899/31/9/N01/meta}{doi:10.1088/0954-3899/31/9/N01}, \href{https://arxiv.org/abs/physics/0306087}{arXiv:physics/0306087}.
	
	\bibitem{ref76} CMS Collaboration, \emph{"CMS Electron and Photon Performance at 13 TeV"}, 2019 J. Phys.: Conf. Ser. 1162 012008, \href{https://iopscience.iop.org/article/10.1088/1742-6596/1162/1/012008}{doi:10.1088/1742-6596/1162/1/012008}.
	% Jets
	\bibitem{ref77} Sirunyan, Albert M and others, \emph{“Identification of heavy-flavour jets with the CMS detector in pp collisions at 13 TeV ”}, JINST, 13 (2018) P05011, \href{https://iopscience.iop.org/article/10.1088/1748-0221/13/05/P05011/meta}{doi: 10.1088/1748- 0221/13/05/P05011}, \href{https://arxiv.org/abs/1712.07158}{arXiv:1712.07158}.
	
	\bibitem{ref78} M. Cacciari, G. P. Salam, and G. Soyez, \emph{“The anti-kt jet clustering algorithm”}, JHEP, vol. 0804, 2008, \href{https://iopscience.iop.org/article/10.1088/1126-6708/2008/04/063}{doi:10.1088/1126-6708/2008/04/063}, \href{https://arxiv.org/abs/0802.1189}{arXiv:0802.1189}.
	
	\bibitem{ref79}
	
		
% Ref Analysis Note	
	\bibitem{ref100} X.-Y. Pham, \emph{“Lepton flavor changing in neutrinoless tau decays”}, Eur. Phys. J. C 8 (1999) 513–516, \href{https://link.springer.com/article/10.1007\%2Fs100529901088}{doi:10.1007/s100529901088}, \href{https://arxiv.org/abs/hep-ph/9810484}{ arXiv:hep-ph/9810484}.
	
	\bibitem{ref101} W. J. Marciano, T. Mori, and J. M. Roney, \emph{“Charged Lepton Flavor Violation Experiments”}, Ann. Rev. Nucl. Part. Sci. 58 (2008) 315, \href{https://www.annualreviews.org/doi/10.1146/annurev.nucl.58.110707.171126}{doi:10.1146/annurev.nucl.58.110707.171126}.

	\bibitem{ref99} S. Mihara, J. P. Miller, P. Paradisi, and G. Piredda, \emph{“Charged Lepton Flavor-Violation Experiments”}, Ann. Rev. Nucl. Part. Sci. 63 (2013) 531–552, \href{https://www.annualreviews.org/doi/10.1146/annurev-nucl-102912-144530}{doi:10.1146/annurev-nucl-102912-144530}.

	\bibitem{ref102} Belle Collaboration, \emph{“Search for Lepton Flavor Violating Tau Decays into Three Leptons with 719 Million Produced Tau+Tau- Pairs”}, Phys. Lett. B 687 (2010) 139, \href{https://www.sciencedirect.com/science/article/pii/S0370269310003576?via\%3Dihub}{doi:10.1016/j.physletb.2010.03.037}, \href{https://arxiv.org/abs/1001.3221}{arXiv:1001.3221}.
	
	\bibitem{ref103} BaBar Collaboration, \emph{“Limits on tau Lepton-Flavor Violating Decays in three charged leptons”}, Phys. Rev. D 81 (2010) 111101, \href{https://journals.aps.org/prd/abstract/10.1103/PhysRevD.81.111101}{doi:10.1103/PhysRevD.81.111101}, \href{https://arxiv.org/abs/1002.4550}{arXiv:1002.4550}.
	
	\bibitem{ref104} T. Sjostrand, S. Mrenna, and P. Z. Skands, \emph{“A Brief Introduction to PYTHIA 8.1”}, Comput. Phys. Commun. 178 (2008) 852, \href{https://www.sciencedirect.com/science/article/pii/S0010465508000441?via\%3Dihub}{doi:10.1016/j.cpc.2008.01.036}, \href{https://arxiv.org/abs/0710.3820}{arXiv:0710.3820}.

	%\bibitem{ref105} LHCb Collaboration, \emph{“The LHCb Detector at the LHC”}, JINST 3 (2008) S08005, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08005}{doi:10.1088/1748-0221/3/08/S08005}. -- Già messo!
	
	%\bibitem{ref106} ATLAS Collaboration, \emph{“The ATLAS Experiment at the CERN Large Hadron Collider”}, JINST 3 (2008) S08003, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08003}{doi:10.1088/1748-0221/3/08/S08003}. -- Già messo!
	
	\bibitem{ref107} LHCb Collaboration, \emph{“Search for the lepton flavour violating decay $\tau^{-} \rightarrow \mu^{-}\mu^{+}\mu^{-}”$}, JHEP 02 (2015) 121, \href{https://link.springer.com/article/10.1007\%2FJHEP02\%282015\%29121}{doi:10.1007/JHEP02(2015)121}, \href{https://arxiv.org/abs/1409.8548}{arXiv:1409.8548}.
	
	\bibitem{ref108} ATLAS Collaboration, \emph{“Probing lepton flavour violation via neutrinoless $\tau\rightarrow 3\mu$ decays with the ATLAS detector”}, \href{https://arxiv.org/abs/1601.03567}{arXiv:1601.03567}.
	
	%\bibitem{ref109} CMS Collaboration, \emph{“The CMS experiment at the CERN LHC”}, JINST 3 (2008) S08004, \href{https://iopscience.iop.org/article/10.1088/1748-0221/3/08/S08004}{doi:10.1088/1748-0221/3/08/S08004}. -- Già messo!

	\bibitem{ref110} CMS Collaboration, \emph{“Event generator tunes obtained from underlying event and multiparton scattering measurements”}, Eur. Phys. J. C 76 (2016), no. 3, 155, \href{https://link.springer.com/article/10.1140\%2Fepjc\%2Fs10052-016-3988-x}{doi:10.1140/epjc/s10052-016-3988-x}, \href{https://arxiv.org/abs/1512.00815}{arXiv:1512.00815}.
	
	\bibitem{ref111} GEANT4 Collaboration, \emph{“GEANT4 — a simulation toolkit”}, Nucl. Instrum. Meth. A 506 (2003) 250, \href{https://www.sciencedirect.com/science/article/pii/S0168900203013688?via\%3Dihub}{doi:10.1016/S0168-9002(03)01368-8}.
	
	\bibitem{ref112} Particle Data Group Collaboration, \emph{“Review of Particle Physics”}, Phys. Rev. D 98 (2018), no. 3, 030001, \href{https://journals.aps.org/prd/abstract/10.1103/PhysRevD.98.030001}{doi:10.1103/PhysRevD.98.030001}. % AGGIORNALA !!!
	
	%\bibitem{ref113} CMS Collaboration, \emph{“Particle-flow reconstruction and global event description with the CMS detector”}, JINST 12 (2017) P10003, \href{https://iopscience.iop.org/article/10.1088/1748-0221/12/10/P10003}{doi:10.1088/1748-0221/12/10/P10003}, \href{https://arxiv.org/abs/1706.04965}{arXiv:1706.04965}.  -- Già messo!
	
	%\bibitem{ref114} CMS Collaboration, \emph{“Performance of the CMS muon detector and muon reconstruction with proton-proton collisions at $\sqrt{s}$=13 TeV"}, JINST 13 (2018) P06015, \href{https://iopscience.iop.org/article/10.1088/1748-0221/13/06/P06015}{doi:10.1088/1748-0221/13/06/P06015}, \href{https://arxiv.org/abs/1804.04528}{arXiv:1804.04528}. -- Già messo!
	
	\bibitem{ref115} CMS Collaboration, \emph{“Measurements of properties of the Higgs boson decaying into the four-lepton final state in pp collisions at $\sqrt{s}$ = 13 TeV"}, JHEP 11 (2017) 047, \href{https://link.springer.com/article/10.1007\%2FJHEP11\%282017\%29047}{doi:10.1007/JHEP11(2017)047}, \href{https://arxiv.org/abs/1706.09936}{arXiv:1706.09936}.
	
	\bibitem{ref116} CMS Collaboration, “Measurement of the inclusive W and Z production cross sections in pp collisions at $\sqrt{s}$= 7 TeV”, JHEP 10 (2011) 132, \href{https://link.springer.com/article/10.1007\%2FJHEP10\%282011\%29132}{doi:10.1007/JHEP10(2011)132}, \href{https://arxiv.org/abs/1107.4789}{arXiv:1107.4789}.

	\bibitem{ref117b} G. Cowan, \emph{“Topics in statistical data analysis for high-energy physics"}, CERN Yellow Report CERN-2010-002, pp.197-218, \href{https://arxiv.org/abs/1012.3589}{arXiv:1012.3589}.

	\bibitem{ref117} A. Hocker et al., \emph{“TMVA - Toolkit for Multivariate Data Analysis”}, PoS ACAT (2007)040, \href{https://arxiv.org/abs/physics/0703039}{arXiv:physics/0703039}.
	
	\bibitem{ref117a} Y. Freund and R.E. Schapire, \emph{“A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting"}, J. of Computer and System Science 55, 119 (1997), \href{https://www.sciencedirect.com/science/article/pii/S002200009791504X}{doi:10.1006/jcss.1997.1504}.
	
	\bibitem{ref117c} Y. Freund and R. E. Schapire, \emph{A short introduction to boosting}, J. Jpn. Soc. Artif. Intell. 14 (1999) 771–780, \url{https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf}.
	
	\bibitem{ref118} T. Junk, \emph{“Confidence level computation for combining searches with small statistics”}, Nucl. Instrum. Meth. A 434 (1999) 435, \href{https://www.sciencedirect.com/science/article/pii/S0168900299004982?via\%3Dihub}{doi:10.1016/S0168-9002(99)00498-2}.

	\bibitem{ref119} A. L. Read, \emph{“Presentation of search results: the CLs technique”}, J. Phys. G: Nucl. Part. Phys. 28 (2002) 2693, \href{https://iopscience.iop.org/article/10.1088/0954-3899/28/10/313}{doi:10.1088/0954-3899/28/10/313}.
	
\end{thebibliography}

\end{document}
